## pointcloud
## railway
## bim
## procedural modeling
## segmentation
### Title: Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment. (arXiv:2308.06299v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06299](http://arxiv.org/abs/2308.06299)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06299] Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment](http://arxiv.org/abs/2308.06299) #segmentation`
* Summary: <p>In this paper, we propose a method for addressing the issue of unnoticed
catastrophic deployment and domain shift in neural networks for semantic
segmentation in autonomous driving. Our approach is based on the idea that deep
learning-based perception for autonomous driving is uncertain and best
represented as a probability distribution. As autonomous vehicles' safety is
paramount, it is crucial for perception systems to recognize when the vehicle
is leaving its operational design domain, anticipate hazardous uncertainty, and
reduce the performance of the perception system. To address this, we propose to
encapsulate the neural network under deployment within an uncertainty
estimation envelope that is based on the epistemic uncertainty estimation
through the Monte Carlo Dropout approach. This approach does not require
modification of the deployed neural network and guarantees expected model
performance. Our defensive perception envelope has the capability to estimate a
neural network's performance, enabling monitoring and notification of entering
domains of reduced neural network performance under deployment. Furthermore,
our envelope is extended by novel methods to improve the application in
deployment settings, including reducing compute expenses and confining
estimation noise. Finally, we demonstrate the applicability of our method for
multiple different potential deployment shifts relevant to autonomous driving,
such as transitions into the night, rainy, or snowy domain. Overall, our
approach shows great potential for application in deployment settings and
enables operational design domain recognition via uncertainty, which allows for
defensive perception, safe state triggers, warning notifications, and feedback
for testing or development and adaptation of the perception stack.
</p>

### Title: R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild. (arXiv:2308.06393v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06393](http://arxiv.org/abs/2308.06393)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06393] R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild](http://arxiv.org/abs/2308.06393) #segmentation`
* Summary: <p>Semantic understanding of roadways is a key enabling factor for safe
autonomous driving. However, existing autonomous driving datasets provide
well-structured urban roads while ignoring unstructured roadways containing
distress, potholes, water puddles, and various kinds of road patches i.e.,
earthen, gravel etc. To this end, we introduce Road Region Segmentation dataset
(R2S100K) -- a large-scale dataset and benchmark for training and evaluation of
road segmentation in aforementioned challenging unstructured roadways. R2S100K
comprises 100K images extracted from a large and diverse set of video sequences
covering more than 1000 KM of roadways. Out of these 100K privacy respecting
images, 14,000 images have fine pixel-labeling of road regions, with 86,000
unlabeled images that can be leveraged through semi-supervised learning
methods. Alongside, we present an Efficient Data Sampling (EDS) based
self-training framework to improve learning by leveraging unlabeled data. Our
experimental results demonstrate that the proposed method significantly
improves learning methods in generalizability and reduces the labeling cost for
semantic segmentation tasks. Our benchmark will be publicly available to
facilitate future research at https://r2s100k.github.io/.
</p>

### Title: TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot. (arXiv:2308.06444v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06444](http://arxiv.org/abs/2308.06444)
* Code URL: [https://github.com/cshan-github/tonguesam](https://github.com/cshan-github/tonguesam)
* Copy Paste: `<input type="checkbox">[[2308.06444] TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot](http://arxiv.org/abs/2308.06444) #segmentation`
* Summary: <p>Tongue segmentation serves as the primary step in automated TCM tongue
diagnosis, which plays a significant role in the diagnostic results. Currently,
numerous deep learning based methods have achieved promising results. However,
most of these methods exhibit mediocre performance on tongues different from
the training set. To address this issue, this paper proposes a universal tongue
segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM
is a large-scale pretrained interactive segmentation model known for its
powerful zero-shot generalization capability. Applying SAM to tongue
segmentation enables the segmentation of various types of tongue images with
zero-shot. In this study, a Prompt Generator based on object detection is
integrated into SAM to enable an end-to-end automated tongue segmentation
method. Experiments demonstrate that TongueSAM achieves exceptional performance
across various of tongue segmentation datasets, particularly under zero-shot.
TongueSAM can be directly applied to other datasets without fine-tuning. As far
as we know, this is the first application of large-scale pretrained model for
tongue segmentation. The project and pretrained model of TongueSAM be publiced
in :https://github.com/cshan-github/TongueSAM.
</p>

### Title: Tiny and Efficient Model for the Edge Detection Generalization. (arXiv:2308.06468v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06468](http://arxiv.org/abs/2308.06468)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06468] Tiny and Efficient Model for the Edge Detection Generalization](http://arxiv.org/abs/2308.06468) #segmentation`
* Summary: <p>Most high-level computer vision tasks rely on low-level image operations as
their initial processes. Operations such as edge detection, image enhancement,
and super-resolution, provide the foundations for higher level image analysis.
In this work we address the edge detection considering three main objectives:
simplicity, efficiency, and generalization since current state-of-the-art
(SOTA) edge detection models are increased in complexity for better accuracy.
To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light
convolutional neural network with only $58K$ parameters, less than $0.2$% of
the state-of-the-art models. Training on the BIPED dataset takes $less than 30
minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model
is easy to train and it quickly converges within very first few epochs, while
the predicted edge-maps are crisp and of high quality. Additionally, we propose
a new dataset to test the generalization of edge detection, which comprises
samples from popular images used in edge detection and image segmentation. The
source code is available in https://github.com/xavysp/TEED.
</p>

### Title: Seed Feature Maps-based CNN Models for LEO Satellite Remote Sensing Services. (arXiv:2308.06515v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06515](http://arxiv.org/abs/2308.06515)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06515] Seed Feature Maps-based CNN Models for LEO Satellite Remote Sensing Services](http://arxiv.org/abs/2308.06515) #segmentation`
* Summary: <p>Deploying high-performance convolutional neural network (CNN) models on
low-earth orbit (LEO) satellites for rapid remote sensing image processing has
attracted significant interest from industry and academia. However, the limited
resources available on LEO satellites contrast with the demands of
resource-intensive CNN models, necessitating the adoption of ground-station
server assistance for training and updating these models. Existing approaches
often require large floating-point operations (FLOPs) and substantial model
parameter transmissions, presenting considerable challenges. To address these
issues, this paper introduces a ground-station server-assisted framework. With
the proposed framework, each layer of the CNN model contains only one learnable
feature map (called the seed feature map) from which other feature maps are
generated based on specific rules. The hyperparameters of these rules are
randomly generated instead of being trained, thus enabling the generation of
multiple feature maps from the seed feature map and significantly reducing
FLOPs. Furthermore, since the random hyperparameters can be saved using a few
random seeds, the ground station server assistance can be facilitated in
updating the CNN model deployed on the LEO satellite. Experimental results on
the ISPRS Vaihingen, ISPRS Potsdam, UAVid, and LoveDA datasets for semantic
segmentation services demonstrate that the proposed framework outperforms
existing state-of-the-art approaches. In particular, the SineFM-based model
achieves a higher mIoU than the UNetFormer on the UAVid dataset, with 3.3x
fewer parameters and 2.2x fewer FLOPs.
</p>

### Title: BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation. (arXiv:2308.06530v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06530](http://arxiv.org/abs/2308.06530)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06530] BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation](http://arxiv.org/abs/2308.06530) #segmentation`
* Summary: <p>Cross-modal Unsupervised Domain Adaptation (UDA) aims to exploit the
complementarity of 2D-3D data to overcome the lack of annotation in a new
domain. However, UDA methods rely on access to the target domain during
training, meaning the trained model only works in a specific target domain. In
light of this, we propose cross-modal learning under bird's-eye view for Domain
Generalization (DG) of 3D semantic segmentation, called BEV-DG. DG is more
challenging because the model cannot access the target domain during training,
meaning it needs to rely on cross-modal learning to alleviate the domain gap.
Since 3D semantic segmentation requires the classification of each point,
existing cross-modal learning is directly conducted point-to-point, which is
sensitive to the misalignment in projections between pixels and points. To this
end, our approach aims to optimize domain-irrelevant representation modeling
with the aid of cross-modal learning under bird's-eye view. We propose
BEV-based Area-to-area Fusion (BAF) to conduct cross-modal learning under
bird's-eye view, which has a higher fault tolerance for point-level
misalignment. Furthermore, to model domain-irrelevant representations, we
propose BEV-driven Domain Contrastive Learning (BDCL) with the help of
cross-modal learning under bird's-eye view. We design three domain
generalization settings based on three 3D datasets, and BEV-DG significantly
outperforms state-of-the-art competitors with tremendous margins in all
settings.
</p>

### Title: SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning. (arXiv:2308.06531v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06531](http://arxiv.org/abs/2308.06531)
* Code URL: [https://github.com/aim-uofa/segprompt](https://github.com/aim-uofa/segprompt)
* Copy Paste: `<input type="checkbox">[[2308.06531] SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning](http://arxiv.org/abs/2308.06531) #segmentation`
* Summary: <p>Current closed-set instance segmentation models rely on pre-defined class
labels for each mask during training and evaluation, largely limiting their
ability to detect novel objects. Open-world instance segmentation (OWIS) models
address this challenge by detecting unknown objects in a class-agnostic manner.
However, previous OWIS approaches completely erase category information during
training to keep the model's ability to generalize to unknown objects. In this
work, we propose a novel training mechanism termed SegPrompt that uses category
information to improve the model's class-agnostic segmentation ability for both
known and unknown categories. In addition, the previous OWIS training setting
exposes the unknown classes to the training set and brings information leakage,
which is unreasonable in the real world. Therefore, we provide a new open-world
benchmark closer to a real-world scenario by dividing the dataset classes into
known-seen-unseen parts. For the first time, we focus on the model's ability to
discover objects that never appear in the training set images.
</p>
<p>Experiments show that SegPrompt can improve the overall and unseen detection
performance by 5.6% and 6.1% in AR on our new benchmark without affecting the
inference efficiency. We further demonstrate the effectiveness of our method on
existing cross-dataset transfer and strongly supervised settings, leading to
5.5% and 12.3% relative improvement.
</p>

### Title: LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06603](http://arxiv.org/abs/2308.06603)
* Code URL: [https://github.com/ach-1914/ladlenet](https://github.com/ach-1914/ladlenet)
* Copy Paste: `<input type="checkbox">[[2308.06603] LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net](http://arxiv.org/abs/2308.06603) #segmentation`
* Summary: <p>The translation of thermal infrared (TIR) images to visible light (VI) images
presents a challenging task with potential applications spanning various
domains such as TIR-VI image registration and fusion. Leveraging supplementary
information derived from TIR image conversions can significantly enhance model
performance and generalization across these applications. However, prevailing
issues within this field include suboptimal image fidelity and limited model
scalability. In this paper, we introduce an algorithm, LadleNet, based on the
U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure,
augmented with skip connections and refined feature aggregation techniques,
resulting in a substantial enhancement in model performance. Comprising
'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the
construction of an abstract semantic space, while the Bowl module decodes this
semantic space to yield mapped VI images. The Handle module exhibits
extensibility by allowing the substitution of its network architecture with
semantic segmentation networks, thereby establishing more abstract semantic
spaces to bolster model performance. Consequently, we propose LadleNet+, which
replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network,
thereby endowing the model with enhanced semantic space construction
capabilities. The proposed method is evaluated and tested on the KAIST dataset,
accompanied by quantitative and qualitative analyses. Compared to existing
methodologies, our approach achieves state-of-the-art performance in terms of
image clarity and perceptual quality. The source code will be made available at
https://github.com/Ach-1914/LadleNet/tree/main/.
</p>

### Title: Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision. (arXiv:2308.06665v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06665](http://arxiv.org/abs/2308.06665)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06665] Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision](http://arxiv.org/abs/2308.06665) #segmentation`
* Summary: <p>Unsupervised Domain Adaptation~(UDA) has attracted a surge of interest over
the past decade but is difficult to be used in real-world applications.
Considering the privacy-preservation issues and security concerns, in this
work, we study a practical problem of Source-Free Domain Adaptation (SFDA),
which eliminates the reliance on annotated source data. Current SFDA methods
focus on extracting domain knowledge from the source-trained model but neglects
the intrinsic structure of the target domain. Moreover, they typically utilize
pseudo labels for self-training in the target domain, but suffer from the
notorious error accumulation problem. To address these issues, we propose a new
SFDA framework, called Region-to-Pixel Adaptation Network~(RPANet), which
learns the region-level and pixel-level discriminative representations through
coarse-to-fine self-supervision. The proposed RPANet consists of two modules,
Foreground-aware Contrastive Learning (FCL) and Confidence-Calibrated
Pseudo-Labeling (CCPL), which explicitly address the key challenges of ``how to
distinguish'' and ``how to refine''. To be specific, FCL introduces a
supervised contrastive learning paradigm in the region level to contrast
different region centroids across different target images, which efficiently
involves all pseudo labels while robust to noisy samples. CCPL designs a novel
fusion strategy to reduce the overconfidence problem of pseudo labels by fusing
two different target predictions without introducing any additional network
modules. Extensive experiments on three cross-domain polyp segmentation tasks
reveal that RPANet significantly outperforms state-of-the-art SFDA and UDA
methods without access to source data, revealing the potential of SFDA in
medical applications.
</p>

### Title: Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation. (arXiv:2308.06693v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06693](http://arxiv.org/abs/2308.06693)
* Code URL: [https://github.com/dlut-yyc/isomer](https://github.com/dlut-yyc/isomer)
* Copy Paste: `<input type="checkbox">[[2308.06693] Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation](http://arxiv.org/abs/2308.06693) #segmentation`
* Summary: <p>Recent leading zero-shot video object segmentation (ZVOS) works devote to
integrating appearance and motion information by elaborately designing feature
fusion modules and identically applying them in multiple feature stages. Our
preliminary experiments show that with the strong long-range dependency
modeling capacity of Transformer, simply concatenating the two modality
features and feeding them to vanilla Transformers for feature fusion can
distinctly benefit the performance but at a cost of heavy computation. Through
further empirical analysis, we find that attention dependencies learned in
Transformer in different stages exhibit completely different properties: global
query-independent dependency in the low-level stages and semantic-specific
dependency in the high-level stages. Motivated by the observations, we propose
two Transformer variants: i) Context-Sharing Transformer (CST) that learns the
global-shared contextual information within image frames with a lightweight
computation. ii) Semantic Gathering-Scattering Transformer (SGST) that models
the semantic correlation separately for the foreground and background and
reduces the computation cost with a soft token merging mechanism. We apply CST
and SGST for low-level and high-level feature fusions, respectively,
formulating a level-isomerous Transformer framework for ZVOS task. Compared
with the baseline that uses vanilla Transformers for multi-stage fusion, ours
significantly increase the speed by 13 times and achieves new state-of-the-art
ZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.
</p>

## 3d point cloud
### Title: Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds. (arXiv:2308.06271v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06271](http://arxiv.org/abs/2308.06271)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06271] Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds](http://arxiv.org/abs/2308.06271) #3d point cloud`
* Summary: <p>Rotational invariance is a popular inductive bias used by many fields in
machine learning, such as computer vision and machine learning for quantum
chemistry. Rotation-invariant machine learning methods set the state of the art
for many tasks, including molecular property prediction and 3D shape
classification. These methods generally either rely on task-specific
rotation-invariant features, or they use general-purpose deep neural networks
which are complicated to design and train. However, it is unclear whether the
success of these methods is primarily due to the rotation invariance or the
deep neural networks. To address this question, we suggest a simple and
general-purpose method for learning rotation-invariant functions of
three-dimensional point cloud data using a random features approach.
Specifically, we extend the random features method of Rahimi &amp; Recht 2007 by
deriving a version that is invariant to three-dimensional rotations and showing
that it is fast to evaluate on point cloud data. We show through experiments
that our method matches or outperforms the performance of general-purpose
rotation-invariant neural networks on standard molecular property prediction
benchmark datasets QM7 and QM9. We also show that our method is general-purpose
and provides a rotation-invariant baseline on the ModelNet40 shape
classification task. Finally, we show that our method has an order of magnitude
smaller prediction latency than competing kernel methods.
</p>

## railway infrastructure
## point cloud segmentation
## extraction
### Title: 4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion. (arXiv:2308.06573v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06573](http://arxiv.org/abs/2308.06573)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06573] 4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion](http://arxiv.org/abs/2308.06573) #extraction`
* Summary: <p>Four-dimensional (4D) radar--visual odometry (4DRVO) integrates complementary
information from 4D radar and cameras, making it an attractive solution for
achieving accurate and robust pose estimation. However, 4DRVO may exhibit
significant tracking errors owing to three main factors: 1) sparsity of 4D
radar point clouds; 2) inaccurate data association and insufficient feature
interaction between the 4D radar and camera; and 3) disturbances caused by
dynamic objects in the environment, affecting odometry estimation. In this
paper, we present 4DRVO-Net, which is a method for 4D radar--visual odometry.
This method leverages the feature pyramid, pose warping, and cost volume (PWC)
network architecture to progressively estimate and refine poses. Specifically,
we propose a multi-scale feature extraction network called Radar-PointNet++
that fully considers rich 4D radar point information, enabling fine-grained
learning for sparse 4D radar point clouds. To effectively integrate the two
modalities, we design an adaptive 4D radar--camera fusion module (A-RCFM) that
automatically selects image features based on 4D radar point features,
facilitating multi-scale cross-modal feature interaction and adaptive
multi-modal feature fusion. In addition, we introduce a velocity-guided
point-confidence estimation module to measure local motion patterns, reduce the
influence of dynamic objects and outliers, and provide continuous updates
during pose refinement. We demonstrate the excellent performance of our method
and the effectiveness of each module design on both the VoD and in-house
datasets. Our method outperforms all learning-based and geometry-based methods
for most sequences in the VoD dataset. Furthermore, it has exhibited promising
performance that closely approaches that of the 64-line LiDAR odometry results
of A-LOAM without mapping optimization.
</p>

### Title: Condition-Adaptive Graph Convolution Learning for Skeleton-Based Gait Recognition. (arXiv:2308.06707v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06707](http://arxiv.org/abs/2308.06707)
* Code URL: [https://github.com/oliverhxh/cag](https://github.com/oliverhxh/cag)
* Copy Paste: `<input type="checkbox">[[2308.06707] Condition-Adaptive Graph Convolution Learning for Skeleton-Based Gait Recognition](http://arxiv.org/abs/2308.06707) #extraction`
* Summary: <p>Graph convolutional networks have been widely applied in skeleton-based gait
recognition. A key challenge in this task is to distinguish the individual
walking styles of different subjects across various views. Existing
state-of-the-art methods employ uniform convolutions to extract features from
diverse sequences and ignore the effects of viewpoint changes. To overcome
these limitations, we propose a condition-adaptive graph (CAG) convolution
network that can dynamically adapt to the specific attributes of each skeleton
sequence and the corresponding view angle. In contrast to using fixed weights
for all joints and sequences, we introduce a joint-specific filter learning
(JSFL) module in the CAG method, which produces sequence-adaptive filters at
the joint level. The adaptive filters capture fine-grained patterns that are
unique to each joint, enabling the extraction of diverse spatial-temporal
information about body parts. Additionally, we design a view-adaptive topology
learning (VATL) module that generates adaptive graph topologies. These graph
topologies are used to correlate the joints adaptively according to the
specific view conditions. Thus, CAG can simultaneously adjust to various
walking styles and viewpoints. Experiments on the two most widely used datasets
(i.e., CASIA-B and OU-MVLP) show that CAG surpasses all previous skeleton-based
methods. Moreover, the recognition performance can be enhanced by simply
combining CAG with appearance-based methods, demonstrating the ability of CAG
to provide useful complementary information.The source code will be available
at https://github.com/OliverHxh/CAG.
</p>

### Title: StairNetV3: Depth-aware Stair Modeling using Deep Learning. (arXiv:2308.06715v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06715](http://arxiv.org/abs/2308.06715)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06715] StairNetV3: Depth-aware Stair Modeling using Deep Learning](http://arxiv.org/abs/2308.06715) #extraction`
* Summary: <p>Vision-based stair perception can help autonomous mobile robots deal with the
challenge of climbing stairs, especially in unfamiliar environments. To address
the problem that current monocular vision methods are difficult to model stairs
accurately without depth information, this paper proposes a depth-aware stair
modeling method for monocular vision. Specifically, we take the extraction of
stair geometric features and the prediction of depth images as joint tasks in a
convolutional neural network (CNN), with the designed information propagation
architecture, we can achieve effective supervision for stair geometric feature
learning by depth information. In addition, to complete the stair modeling, we
take the convex lines, concave lines, tread surfaces and riser surfaces as
stair geometric features and apply Gaussian kernels to enable the network to
predict contextual information within the stair lines. Combined with the depth
information obtained by depth sensors, we propose a stair point cloud
reconstruction method that can quickly get point clouds belonging to the stair
step surfaces. Experiments on our dataset show that our method has a
significant improvement over the previous best monocular vision method, with an
intersection over union (IOU) increase of 3.4 %, and the lightweight version
has a fast detection speed and can meet the requirements of most real-time
applications. Our dataset is available at
https://data.mendeley.com/datasets/6kffmjt7g2/1.
</p>

## lidar
## Infrastructure information models
## edge regularization
## lod
### Title: Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training. (arXiv:2308.06689v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06689](http://arxiv.org/abs/2308.06689)
* Code URL: [https://github.com/dravenalg/reste](https://github.com/dravenalg/reste)
* Copy Paste: `<input type="checkbox">[[2308.06689] Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training](http://arxiv.org/abs/2308.06689) #lod`
* Summary: <p>Binarization of neural networks is a dominant paradigm in neural networks
compression. The pioneering work BinaryConnect uses Straight Through Estimator
(STE) to mimic the gradients of the sign function, but it also causes the
crucial inconsistency problem. Most of the previous methods design different
estimators instead of STE to mitigate it. However, they ignore the fact that
when reducing the estimating error, the gradient stability will decrease
concomitantly. These highly divergent gradients will harm the model training
and increase the risk of gradient vanishing and gradient exploding. To fully
take the gradient stability into consideration, we present a new perspective to
the BNNs training, regarding it as the equilibrium between the estimating error
and the gradient stability. In this view, we firstly design two indicators to
quantitatively demonstrate the equilibrium phenomenon. In addition, in order to
balance the estimating error and the gradient stability well, we revise the
original straight through estimator and propose a power function based
estimator, Rectified Straight Through Estimator (ReSTE for short). Comparing to
other estimators, ReSTE is rational and capable of flexibly balancing the
estimating error with the gradient stability. Extensive experiments on CIFAR-10
and ImageNet datasets show that ReSTE has excellent performance and surpasses
the state-of-the-art methods without any auxiliary modules or losses.
</p>

