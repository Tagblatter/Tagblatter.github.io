## pointcloud
## railway
## bim
## procedural modeling
## segmentation
### Title: RaBiT: An Efficient Transformer using Bidirectional Feature Pyramid Network with Reverse Attention for Colon Polyp Segmentation. (arXiv:2307.06420v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06420](http://arxiv.org/abs/2307.06420)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06420] RaBiT: An Efficient Transformer using Bidirectional Feature Pyramid Network with Reverse Attention for Colon Polyp Segmentation](http://arxiv.org/abs/2307.06420) #segmentation`
* Summary: <p>Automatic and accurate segmentation of colon polyps is essential for early
diagnosis of colorectal cancer. Advanced deep learning models have shown
promising results in polyp segmentation. However, they still have limitations
in representing multi-scale features and generalization capability. To address
these issues, this paper introduces RaBiT, an encoder-decoder model that
incorporates a lightweight Transformer-based architecture in the encoder to
model multiple-level global semantic relationships. The decoder consists of
several bidirectional feature pyramid layers with reverse attention modules to
better fuse feature maps at various levels and incrementally refine polyp
boundaries. We also propose ideas to lighten the reverse attention module and
make it more suitable for multi-class segmentation. Extensive experiments on
several benchmark datasets show that our method outperforms existing methods
across all datasets while maintaining low computational complexity. Moreover,
our method demonstrates high generalization capability in cross-dataset
experiments, even when the training and test sets have different
characteristics.
</p>

### Title: WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmark for Autonomous Driving on Water Surfaces. (arXiv:2307.06505v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06505](http://arxiv.org/abs/2307.06505)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06505] WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmark for Autonomous Driving on Water Surfaces](http://arxiv.org/abs/2307.06505) #segmentation`
* Summary: <p>Autonomous driving on water surfaces plays an essential role in executing
hazardous and time-consuming missions, such as maritime surveillance, survivors
rescue, environmental monitoring, hydrography mapping and waste cleaning. This
work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset
for autonomous driving on water surfaces. Equipped with a 4D radar and a
monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather
solutions for discerning object-related information, including color, shape,
texture, range, velocity, azimuth, and elevation. Focusing on typical static
and dynamic objects on water surfaces, we label the camera images and radar
point clouds at pixel-level and point-level, respectively. In addition to basic
perception tasks, such as object detection, instance segmentation and semantic
segmentation, we also provide annotations for free-space segmentation and
waterline segmentation. Leveraging the multi-task and multi-modal data, we
conduct numerous experiments on the single modality of radar and camera, as
well as the fused modalities. Results demonstrate that 4D radar-camera fusion
can considerably enhance the robustness of perception on water surfaces,
especially in adverse lighting and weather conditions. WaterScenes dataset is
public on https://waterscenes.github.io.
</p>

### Title: RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation. (arXiv:2307.06577v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06577](http://arxiv.org/abs/2307.06577)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06577] RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation](http://arxiv.org/abs/2307.06577) #segmentation`
* Summary: <p>Retinal vessel segmentation is generally grounded in image-based datasets
collected with bench-top devices. The static images naturally lose the dynamic
characteristics of retina fluctuation, resulting in diminished dataset
richness, and the usage of bench-top devices further restricts dataset
scalability due to its limited accessibility. Considering these limitations, we
introduce the first video-based retinal dataset by employing handheld devices
for data acquisition. The dataset comprises 635 smartphone-based fundus videos
collected from four different clinics, involving 415 patients from 50 to 75
years old. It delivers comprehensive and precise annotations of retinal
structures in both spatial and temporal dimensions, aiming to advance the
landscape of vasculature segmentation. Specifically, the dataset provides three
levels of spatial annotations: binary vessel masks for overall retinal
structure delineation, general vein-artery masks for distinguishing the vein
and artery, and fine-grained vein-artery masks for further characterizing the
granularities of each artery and vein. In addition, the dataset offers temporal
annotations that capture the vessel pulsation characteristics, assisting in
detecting ocular diseases that require fine-grained recognition of hemodynamic
fluctuation. In application, our dataset exhibits a significant domain shift
with respect to data captured by bench-top devices, thus posing great
challenges to existing methods. In the experiments, we provide evaluation
metrics and benchmark results on our dataset, reflecting both the potential and
challenges it offers for vessel segmentation tasks. We hope this challenging
dataset would significantly contribute to the development of eye disease
diagnosis and early prevention.
</p>

### Title: YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices. (arXiv:2307.06689v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06689](http://arxiv.org/abs/2307.06689)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06689] YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices](http://arxiv.org/abs/2307.06689) #segmentation`
* Summary: <p>In the realm of Tiny AI, we introduce "You Only Look at Interested Cells"
(YOLIC), an efficient method for object localization and classification on edge
devices. Seamlessly blending the strengths of semantic segmentation and object
detection, YOLIC offers superior computational efficiency and precision. By
adopting Cells of Interest for classification instead of individual pixels,
YOLIC encapsulates relevant information, reduces computational load, and
enables rough object shape inference. Importantly, the need for bounding box
regression is obviated, as YOLIC capitalizes on the predetermined cell
configuration that provides information about potential object location, size,
and shape. To tackle the issue of single-label classification limitations, a
multi-label classification approach is applied to each cell, effectively
recognizing overlapping or closely situated objects. This paper presents
extensive experiments on multiple datasets, demonstrating that YOLIC achieves
detection performance comparable to the state-of-the-art YOLO algorithms while
surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resources
related to this study, including datasets, cell designer, image annotation
tool, and source code, have been made publicly available on our project website
at https://kai3316.github.io/yolic.github.io
</p>

## 3d point cloud
## railway infrastructure
## point cloud segmentation
## extraction
### Title: Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023). (arXiv:2307.06396v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06396](http://arxiv.org/abs/2307.06396)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06396] Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023)](http://arxiv.org/abs/2307.06396) #extraction`
* Summary: <p>The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment. FMER is a subset of image processing and it
is a multidisciplinary topic to analysis. So, it requires familiarity with
other topics of Artifactual Intelligence (AI) such as machine learning, digital
image processing, psychology and more. So, it is a great opportunity to write a
book which covers all of these topics for beginner to professional readers in
the field of AI and even without having background of AI. Our goal is to
provide a standalone introduction in the field of MFER analysis in the form of
theorical descriptions for readers with no background in image processing with
reproducible Matlab practical examples. Also, we describe any basic definitions
for FMER analysis and MATLAB library which is used in the text, that helps
final reader to apply the experiments in the real-world applications. We
believe that this book is suitable for students, researchers, and professionals
alike, who need to develop practical skills, along with a basic understanding
of the field. We expect that, after reading this book, the reader feels
comfortable with different key stages such as color and depth image processing,
color and depth image representation, classification, machine learning, facial
micro-expressions recognition, feature extraction and dimensionality reduction.
The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment.
</p>

### Title: A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023. (arXiv:2307.06569v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06569](http://arxiv.org/abs/2307.06569)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06569] A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023](http://arxiv.org/abs/2307.06569) #extraction`
* Summary: <p>In this technical report, we present our findings from a study conducted on
the EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action
Recognition. Our research focuses on the innovative application of a
differentiable logic loss in the training to leverage the co-occurrence
relations between verb and noun, as well as the pre-trained Large Language
Models (LLMs) to generate the logic rules for the adaptation to unseen action
labels. Specifically, the model's predictions are treated as the truth
assignment of a co-occurrence logic formula to compute the logic loss, which
measures the consistency between the predictions and the logic constraints. By
using the verb-noun co-occurrence matrix generated from the dataset, we observe
a moderate improvement in model performance compared to our baseline framework.
To further enhance the model's adaptability to novel action labels, we
experiment with rules generated using GPT-3.5, which leads to a slight decrease
in performance. These findings shed light on the potential and challenges of
incorporating differentiable logic and LLMs for knowledge extraction in
unsupervised domain adaptation for action recognition. Our final submission
(entitled `NS-LLM') achieved the first place in terms of top-1 action
recognition accuracy.
</p>

### Title: DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification. (arXiv:2307.06667v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.06667](http://arxiv.org/abs/2307.06667)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.06667] DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification](http://arxiv.org/abs/2307.06667) #extraction`
* Summary: <p>Deep neural networks face many problems in the field of hyperspectral image
classification, lack of effective utilization of spatial spectral information,
gradient disappearance and overfitting as the model depth increases. In order
to accelerate the deployment of the model on edge devices with strict latency
requirements and limited computing power, we introduce a lightweight model
based on the improved 3D-Densenet model and designs DGCNet. It improves the
disadvantage of group convolution. Referring to the idea of dynamic network,
dynamic group convolution(DGC) is designed on 3d convolution kernel. DGC
introduces small feature selectors for each grouping to dynamically decide
which part of the input channel to connect based on the activations of all
input channels. Multiple groups can capture different and complementary visual
and semantic features of input images, allowing convolution neural network(CNN)
to learn rich features. 3D convolution extracts high-dimensional and redundant
hyperspectral data, and there is also a lot of redundant information between
convolution kernels. DGC module allows 3D-Densenet to select channel
information with richer semantic features and discard inactive regions. The
3D-CNN passing through the DGC module can be regarded as a pruned network. DGC
not only allows 3D-CNN to complete sufficient feature extraction, but also
takes into account the requirements of speed and calculation amount. The
inference speed and accuracy have been improved, with outstanding performance
on the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral image
classification methods.
</p>

## lidar
## Infrastructure information models
## edge regularization
## lod
