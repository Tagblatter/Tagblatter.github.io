## pointcloud
## railway
## bim
## procedural modeling
## segmentation
### Title: On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild. (arXiv:2307.10267v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10267](http://arxiv.org/abs/2307.10267)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10267] On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild](http://arxiv.org/abs/2307.10267) #segmentation`
* Summary: <p>Aphid infestations can cause extensive damage to wheat and sorghum fields and
spread plant viruses, resulting in significant yield losses in agriculture. To
address this issue, farmers often rely on chemical pesticides, which are
inefficiently applied over large areas of fields. As a result, a considerable
amount of pesticide is wasted on areas without pests, while inadequate amounts
are applied to areas with severe infestations. The paper focuses on the urgent
need for an intelligent autonomous system that can locate and spray
infestations within complex crop canopies, reducing pesticide use and
environmental impact. We have collected and labeled a large aphid image dataset
in the field, and propose the use of real-time semantic segmentation models to
segment clusters of aphids. A multiscale dataset is generated to allow for
learning the clusters at different scales. We compare the segmentation speeds
and accuracy of four state-of-the-art real-time semantic segmentation models on
the aphid cluster dataset, benchmarking them against nonreal-time models. The
study results show the effectiveness of a real-time solution, which can reduce
inefficient pesticide use and increase crop yields, paving the way towards an
autonomous pest detection system.
</p>

### Title: CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation. (arXiv:2307.10316v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10316](http://arxiv.org/abs/2307.10316)
* Code URL: [https://github.com/lizhaoliu-Lec/CPCM](https://github.com/lizhaoliu-Lec/CPCM)
* Copy Paste: `<input type="checkbox">[[2307.10316] CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation](http://arxiv.org/abs/2307.10316) #segmentation`
* Summary: <p>We study the task of weakly-supervised point cloud semantic segmentation with
sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce
the expensive cost of dense annotations. Unfortunately, with extremely sparse
annotated points, it is very difficult to extract both contextual and object
information for scene understanding such as semantic segmentation. Motivated by
masked modeling (e.g., MAE) in image and video representation learning, we seek
to endow the power of masked modeling to learn contextual information from
sparsely-annotated points. However, directly applying MAE to 3D point clouds
with sparse annotations may fail to work. First, it is nontrivial to
effectively mask out the informative visual context from 3D point clouds.
Second, how to fully exploit the sparse annotations for context modeling
remains an open question. In this paper, we propose a simple yet effective
Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a
region-wise masking (RegionMask) strategy and a contextual masked training
(CMT) method. Specifically, RegionMask masks the point cloud continuously in
geometric space to construct a meaningful masked prediction task for subsequent
context learning. CMT disentangles the learning of supervised segmentation and
unsupervised masked context prediction for effectively learning the very
limited labeled points and mass unlabeled points, respectively. Extensive
experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate
the superiority of CPCM over the state-of-the-art.
</p>

### Title: POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities. (arXiv:2307.10387v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10387](http://arxiv.org/abs/2307.10387)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10387] POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities](http://arxiv.org/abs/2307.10387) #segmentation`
* Summary: <p>The surgical usage of Mixed Reality (MR) has received growing attention in
areas such as surgical navigation systems, skill assessment, and robot-assisted
surgeries. For such applications, pose estimation for hand and surgical
instruments from an egocentric perspective is a fundamental task and has been
studied extensively in the computer vision field in recent years. However, the
development of this field has been impeded by a lack of datasets, especially in
the surgical field, where bloody gloves and reflective metallic tools make it
hard to obtain 3D pose annotations for hands and objects using conventional
methods. To address this issue, we propose POV-Surgery, a large-scale,
synthetic, egocentric dataset focusing on pose estimation for hands with
different surgical gloves and three orthopedic surgical instruments, namely
scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329
frames, featuring high-resolution RGB-D video streams with activity
annotations, accurate 3D and 2D annotations for hand-object pose, and 2D
hand-object segmentation masks. We fine-tune the current SOTA methods on
POV-Surgery and further show the generalizability when applying to real-life
cases with surgical gloves and tools by extensive evaluations. The code and the
dataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.
</p>

### Title: Interactive Segmentation for Diverse Gesture Types Without Context. (arXiv:2307.10518v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10518](http://arxiv.org/abs/2307.10518)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10518] Interactive Segmentation for Diverse Gesture Types Without Context](http://arxiv.org/abs/2307.10518) #segmentation`
* Summary: <p>Interactive segmentation entails a human marking an image to guide how a
model either creates or edits a segmentation. Our work addresses limitations of
existing methods: they either only support one gesture type for marking an
image (e.g., either clicks or scribbles) or require knowledge of the gesture
type being employed, and require specifying whether marked regions should be
included versus excluded in the final segmentation. We instead propose a
simplified interactive segmentation task where a user only must mark an image,
where the input can be of any gesture type without specifying the gesture type.
We support this new task by introducing the first interactive segmentation
dataset with multiple gesture types as well as a new evaluation metric capable
of holistically evaluating interactive segmentation algorithms. We then analyze
numerous interactive segmentation algorithms, including ones adapted for our
novel task. While we observe promising performance overall, we also highlight
areas for future improvement. To facilitate further extensions of this work, we
publicly share our new dataset at https://github.com/joshmyersdean/dig.
</p>

### Title: Quantized Feature Distillation for Network Quantization. (arXiv:2307.10638v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10638](http://arxiv.org/abs/2307.10638)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10638] Quantized Feature Distillation for Network Quantization](http://arxiv.org/abs/2307.10638) #segmentation`
* Summary: <p>Neural network quantization aims to accelerate and trim full-precision neural
network models by using low bit approximations. Methods adopting the
quantization aware training (QAT) paradigm have recently seen a rapid growth,
but are often conceptually complicated. This paper proposes a novel and highly
effective QAT method, quantized feature distillation (QFD). QFD first trains a
quantized (or binarized) representation as the teacher, then quantize the
network using knowledge distillation (KD). Quantitative results show that QFD
is more flexible and effective (i.e., quantization friendly) than previous
quantization methods. QFD surpasses existing methods by a noticeable margin on
not only image classification but also object detection, albeit being much
simpler. Furthermore, QFD quantizes ViT and Swin-Transformer on MS-COCO
detection and segmentation, which verifies its potential in real world
deployment. To the best of our knowledge, this is the first time that vision
transformers have been quantized in object detection and image segmentation
tasks.
</p>

### Title: TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10705](http://arxiv.org/abs/2307.10705)
* Code URL: [https://github.com/chequanghuy/TwinLiteNet](https://github.com/chequanghuy/TwinLiteNet)
* Copy Paste: `<input type="checkbox">[[2307.10705] TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars](http://arxiv.org/abs/2307.10705) #segmentation`
* Summary: <p>Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
</p>

### Title: EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation. (arXiv:2307.10745v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10745](http://arxiv.org/abs/2307.10745)
* Code URL: [https://github.com/mak-ta-reque/edgeal](https://github.com/mak-ta-reque/edgeal)
* Copy Paste: `<input type="checkbox">[[2307.10745] EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation](http://arxiv.org/abs/2307.10745) #segmentation`
* Summary: <p>Active learning algorithms have become increasingly popular for training
models with limited data. However, selecting data for annotation remains a
challenging problem due to the limited information available on unseen data. To
address this issue, we propose EdgeAL, which utilizes the edge information of
unseen images as {\it a priori} information for measuring uncertainty. The
uncertainty is quantified by analyzing the divergence and entropy in model
predictions across edges. This measure is then used to select superpixels for
annotation. We demonstrate the effectiveness of EdgeAL on multi-class Optical
Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice
score while reducing the annotation label cost to 12%, 2.3%, and 3%,
respectively, on three publicly available datasets (Duke, AROI, and UMN). The
source code is available at \url{https://github.com/Mak-Ta-Reque/EdgeAL}
</p>

### Title: See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data. (arXiv:2307.10782v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10782](http://arxiv.org/abs/2307.10782)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10782] See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data](http://arxiv.org/abs/2307.10782) #segmentation`
* Summary: <p>Zero-shot point cloud segmentation aims to make deep models capable of
recognizing novel objects in point cloud that are unseen in the training phase.
Recent trends favor the pipeline which transfers knowledge from seen classes
with labels to unseen classes without labels. They typically align visual
features with semantic features obtained from word embedding by the supervision
of seen classes' annotations. However, point cloud contains limited information
to fully match with semantic features. In fact, the rich appearance information
of images is a natural complement to the textureless point cloud, which is not
well explored in previous literature. Motivated by this, we propose a novel
multi-modal zero-shot learning method to better utilize the complementary
information of point clouds and images for more accurate visual-semantic
alignment. Extensive experiments are performed in two popular benchmarks, i.e.,
SemanticKITTI and nuScenes, and our method outperforms current SOTA methods
with 52% and 49% improvement on average for unseen class mIoU, respectively.
</p>

### Title: Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10792](http://arxiv.org/abs/2307.10792)
* Code URL: [https://github.com/scortexio/patchcore-few-shot](https://github.com/scortexio/patchcore-few-shot)
* Copy Paste: `<input type="checkbox">[[2307.10792] Optimizing PatchCore for Few/many-shot Anomaly Detection](http://arxiv.org/abs/2307.10792) #segmentation`
* Summary: <p>Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and
tries to distinguish between normal and anomalous data using only few selected
samples. While newly proposed few-shot AD methods do compare against
pre-existing algorithms developed for the full-shot domain as baselines, they
do not dedicatedly optimize them for the few-shot setting. It thus remains
unclear if the performance of such pre-existing algorithms can be further
improved. We address said question in this work. Specifically, we present a
study on the AD/anomaly segmentation (AS) performance of PatchCore, the current
state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the
many-shot settings. We hypothesize that further performance improvements can be
realized by (I) optimizing its various hyperparameters, and by (II)
transferring techniques known to improve few-shot supervised learning to the AD
domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal
that (I) significant performance improvements can be realized by optimizing
hyperparameters such as the underlying feature extractor, and that (II)
image-level augmentations can, but are not guaranteed, to improve performance.
Based on these findings, we achieve a new state of the art in few-shot AD on
VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to
the few-shot setting. Last, we identify the investigation of feature extractors
with a strong inductive bias as a potential future research direction for
(few-shot) AD/AS.
</p>

### Title: Gradient-Semantic Compensation for Incremental Semantic Segmentation. (arXiv:2307.10822v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10822](http://arxiv.org/abs/2307.10822)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10822] Gradient-Semantic Compensation for Incremental Semantic Segmentation](http://arxiv.org/abs/2307.10822) #segmentation`
* Summary: <p>Incremental semantic segmentation aims to continually learn the segmentation
of new coming classes without accessing the training data of previously learned
classes. However, most current methods fail to address catastrophic forgetting
and background shift since they 1) treat all previous classes equally without
considering different forgetting paces caused by imbalanced gradient
back-propagation; 2) lack strong semantic guidance between classes. To tackle
the above challenges, in this paper, we propose a Gradient-Semantic
Compensation (GSC) model, which surmounts incremental semantic segmentation
from both gradient and semantic perspectives. Specifically, to address
catastrophic forgetting from the gradient aspect, we develop a step-aware
gradient compensation that can balance forgetting paces of previously seen
classes via re-weighting gradient backpropagation. Meanwhile, we propose a
soft-sharp semantic relation distillation to distill consistent inter-class
semantic relations via soft labels for alleviating catastrophic forgetting from
the semantic aspect. In addition, we develop a prototypical pseudo re-labeling
that provides strong semantic guidance to mitigate background shift. It
produces high-quality pseudo labels for old classes in the background by
measuring distances between pixels and class-wise prototypes. Extensive
experiments on three public datasets, i.e., Pascal VOC 2012, ADE20K, and
Cityscapes, demonstrate the effectiveness of our proposed GSC model.
</p>

### Title: Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10842](http://arxiv.org/abs/2307.10842)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10842] Label Calibration for Semantic Segmentation Under Domain Shift](http://arxiv.org/abs/2307.10842) #segmentation`
* Summary: <p>Performance of a pre-trained semantic segmentation model is likely to
substantially decrease on data from a new domain. We show a pre-trained model
can be adapted to unlabelled target domain data by calculating soft-label
prototypes under the domain shift and making predictions according to the
prototype closest to the vector with predicted class probabilities. The
proposed adaptation procedure is fast, comes almost for free in terms of
computational resources and leads to considerable performance improvements. We
demonstrate the benefits of such label calibration on the highly-practical
synthetic-to-real semantic segmentation problem.
</p>

## 3d point cloud
## railway infrastructure
## point cloud segmentation
## extraction
### Title: No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection. (arXiv:2307.10567v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10567](http://arxiv.org/abs/2307.10567)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10567] No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection](http://arxiv.org/abs/2307.10567) #extraction`
* Summary: <p>Temporal video grounding (TVG) aims to retrieve the time interval of a
language query from an untrimmed video. A significant challenge in TVG is the
low "Semantic Noise Ratio (SNR)", which results in worse performance with lower
SNR. Prior works have addressed this challenge using sophisticated techniques.
In this paper, we propose a no-frills TVG model that consists of two core
modules, namely multi-scale neighboring attention and zoom-in boundary
detection. The multi-scale neighboring attention restricts each video token to
only aggregate visual contexts from its neighbor, enabling the extraction of
the most distinguishing information with multi-scale feature hierarchies from
high-ratio noises. The zoom-in boundary detection then focuses on local-wise
discrimination of the selected top candidates for fine-grained grounding
adjustment. With an end-to-end training strategy, our model achieves
competitive performance on different TVG benchmarks, while also having the
advantage of faster inference speed and lighter model parameters, thanks to its
lightweight architecture.
</p>

### Title: Hybrid Feature Embedding For Automatic Building Outline Extraction. (arXiv:2307.10609v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10609](http://arxiv.org/abs/2307.10609)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10609] Hybrid Feature Embedding For Automatic Building Outline Extraction](http://arxiv.org/abs/2307.10609) #extraction`
* Summary: <p>Building outline extracted from high-resolution aerial images can be used in
various application fields such as change detection and disaster assessment.
However, traditional CNN model cannot recognize contours very precisely from
original images. In this paper, we proposed a CNN and Transformer based model
together with active contour model to deal with this problem. We also designed
a triple-branch decoder structure to handle different features generated by
encoder. Experiment results show that our model outperforms other baseline
model on two datasets, achieving 91.1% mIoU on Vaihingen and 83.8% on Bing
huts.
</p>

### Title: Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10695](http://arxiv.org/abs/2307.10695)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10695] Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss](http://arxiv.org/abs/2307.10695) #extraction`
* Summary: <p>Recently, denoising methods based on supervised learning have exhibited
promising performance. However, their reliance on external datasets containing
noisy-clean image pairs restricts their applicability. To address this
limitation, researchers have focused on training denoising networks using
solely a set of noisy inputs. To improve the feasibility of denoising
procedures, in this study, we proposed a single-image self-supervised learning
method in which only the noisy input image is used for network training. Gated
convolution was used for feature extraction and no-reference image quality
assessment was used for guiding the training process. Moreover, the proposed
method sampled instances from the input image dataset using Bernoulli sampling
with a certain dropout rate for training. The corresponding result was produced
by averaging the generated predictions from various instances of the trained
network with dropouts. The experimental results indicated that the proposed
method achieved state-of-the-art denoising performance on both synthetic and
real-world datasets. This highlights the effectiveness and practicality of our
method as a potential solution for various noise removal tasks.
</p>

## lidar
### Title: RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection. (arXiv:2307.10249v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2307.10249](http://arxiv.org/abs/2307.10249)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2307.10249] RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection](http://arxiv.org/abs/2307.10249) #lidar`
* Summary: <p>While LiDAR sensors have been succesfully applied to 3D object detection, the
affordability of radar and camera sensors has led to a growing interest in
fusiong radars and cameras for 3D object detection. However, previous
radar-camera fusion models have not been able to fully utilize radar
information in that initial 3D proposals were generated based on the camera
features only and the instance-level fusion is subsequently conducted. In this
paper, we propose radar-camera multi-level fusion (RCM-Fusion), which fuses
radar and camera modalities at both the feature-level and instance-level to
fully utilize radar information. At the feature-level, we propose a Radar
Guided BEV Encoder which utilizes radar Bird's-Eye-View (BEV) features to
transform image features into precise BEV representations and then adaptively
combines the radar and camera BEV features. At the instance-level, we propose a
Radar Grid Point Refinement module that reduces localization error by
considering the characteristics of the radar point clouds. The experiments
conducted on the public nuScenes dataset demonstrate that our proposed
RCM-Fusion offers 11.8% performance gain in nuScenes detection score (NDS) over
the camera-only baseline model and achieves state-of-the-art performaces among
radar-camera fusion methods in the nuScenes 3D object detection benchmark. Code
will be made publicly available.
</p>

## Infrastructure information models
## edge regularization
## lod
