<h2>pointcloud</h2>
<h2>railway</h2>
<h2>BIM</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking. (arXiv:2307.02508v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02508">http://arxiv.org/abs/2307.02508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02508] ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking](http://arxiv.org/abs/2307.02508) #segmentation</code></li>
<li>Summary: <p>The Associating Objects with Transformers (AOT) framework has exhibited
exceptional performance in a wide range of complex scenarios for video object
tracking and segmentation. In this study, we convert the bounding boxes to
masks in reference frames with the help of the Segment Anything Model (SAM) and
Alpha-Refine, and then propagate the masks to the current frame, transforming
the task from Video Object Tracking (VOT) to video object segmentation (VOS).
Furthermore, we introduce MSDeAOT, a variant of the AOT series that
incorporates transformers at multiple feature scales. MSDeAOT efficiently
propagates object masks from previous frames to the current frame using two
feature scales of 16 and 8. As a testament to the effectiveness of our design,
we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking
Challenge.
</p></li>
</ul>
<h3>Title: GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging. (arXiv:2307.02595v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02595">http://arxiv.org/abs/2307.02595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02595] GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging](http://arxiv.org/abs/2307.02595) #segmentation</code></li>
<li>Summary: <p>This paper explores the application of event-based cameras in the domains of
image segmentation and motion estimation. These cameras offer a groundbreaking
technology by capturing visual information as a continuous stream of
asynchronous events, departing from the conventional frame-based image
acquisition. We introduce a Generalized Nash Equilibrium based framework that
leverages the temporal and spatial information derived from the event stream to
carry out segmentation and velocity estimation. To establish the theoretical
foundations, we derive an existence criteria and propose a multi-level
optimization method for calculating equilibrium. The efficacy of this approach
is shown through a series of experiments.
</p></li>
</ul>
<h3>Title: Spherical Feature Pyramid Networks For Semantic Segmentation. (arXiv:2307.02658v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02658">http://arxiv.org/abs/2307.02658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02658] Spherical Feature Pyramid Networks For Semantic Segmentation](http://arxiv.org/abs/2307.02658) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation for spherical data is a challenging problem in machine
learning since conventional planar approaches require projecting the spherical
image to the Euclidean plane. Representing the signal on a fundamentally
different topology introduces edges and distortions which impact network
performance. Recently, graph-based approaches have bypassed these challenges to
attain significant improvements by representing the signal on a spherical mesh.
Current approaches to spherical segmentation exclusively use variants of the
UNet architecture, meaning more successful planar architectures remain
unexplored. Inspired by the success of feature pyramid networks (FPNs) in
planar image segmentation, we leverage the pyramidal hierarchy of graph-based
spherical CNNs to design spherical FPNs. Our spherical FPN models show
consistent improvements over spherical UNets, whilst using fewer parameters. On
the Stanford 2D-3D-S dataset, our models achieve state-of-the-art performance
with an mIOU of 48.75, an improvement of 3.75 IoU points over the previous best
spherical CNN.
</p></li>
</ul>
<h3>Title: Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning. (arXiv:2307.02798v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02798">http://arxiv.org/abs/2307.02798</a></li>
<li>Code URL: <a href="https://github.com/hritam-98/gfda-disentangled">https://github.com/hritam-98/gfda-disentangled</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02798] Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning](http://arxiv.org/abs/2307.02798) #segmentation</code></li>
<li>Summary: <p>Although unsupervised domain adaptation (UDA) is a promising direction to
alleviate domain shift, they fall short of their supervised counterparts. In
this work, we investigate relatively less explored semi-supervised domain
adaptation (SSDA) for medical image segmentation, where access to a few labeled
target samples can improve the adaptation performance substantially.
Specifically, we propose a two-stage training process. First, an encoder is
pre-trained in a self-learning paradigm using a novel domain-content
disentangled contrastive learning (CL) along with a pixel-level feature
consistency constraint. The proposed CL enforces the encoder to learn
discriminative content-specific but domain-invariant semantics on a global
scale from the source and target images, whereas consistency regularization
enforces the mining of local pixel-level information by maintaining spatial
sensitivity. This pre-trained encoder, along with a decoder, is further
fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a
semi-supervised setting. Furthermore, we experimentally validate that our
proposed method can easily be extended for UDA settings, adding to the
superiority of the proposed strategy. Upon evaluation on two domain adaptive
image segmentation tasks, our proposed method outperforms the SoTA methods,
both in SSDA and UDA settings. Code is available at
https://github.com/hritam-98/GFDA-disentangled
</p></li>
</ul>
<h3>Title: A Critical Look at the Current Usage of Foundation Model for Dense Recognition Task. (arXiv:2307.02862v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02862">http://arxiv.org/abs/2307.02862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02862] A Critical Look at the Current Usage of Foundation Model for Dense Recognition Task](http://arxiv.org/abs/2307.02862) #segmentation</code></li>
<li>Summary: <p>In recent years large model trained on huge amount of cross-modality data,
which is usually be termed as foundation model, achieves conspicuous
accomplishment in many fields, such as image recognition and generation. Though
achieving great success in their original application case, it is still unclear
whether those foundation models can be applied to other different downstream
tasks. In this paper, we conduct a short survey on the current methods for
discriminative dense recognition tasks, which are built on the pretrained
foundation model. And we also provide some preliminary experimental analysis of
an existing open-vocabulary segmentation method based on Stable Diffusion,
which indicates the current way of deploying diffusion model for segmentation
is not optimal. This aims to provide insights for future research on adopting
foundation model for downstream task.
</p></li>
</ul>
<h3>Title: Towards accurate instance segmentation in large-scale LiDAR point clouds. (arXiv:2307.02877v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02877">http://arxiv.org/abs/2307.02877</a></li>
<li>Code URL: <a href="https://github.com/bxiang233/panopticsegforlargescalepointcloud">https://github.com/bxiang233/panopticsegforlargescalepointcloud</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02877] Towards accurate instance segmentation in large-scale LiDAR point clouds](http://arxiv.org/abs/2307.02877) #segmentation</code></li>
<li>Summary: <p>Panoptic segmentation is the combination of semantic and instance
segmentation: assign the points in a 3D point cloud to semantic categories and
partition them into distinct object instances. It has many obvious applications
for outdoor scene understanding, from city mapping to forest management.
Existing methods struggle to segment nearby instances of the same semantic
category, like adjacent pieces of street furniture or neighbouring trees, which
limits their usability for inventory- or management-type applications that rely
on object instances. This study explores the steps of the panoptic segmentation
pipeline concerned with clustering points into object instances, with the goal
to alleviate that bottleneck. We find that a carefully designed clustering
strategy, which leverages multiple types of learned point embeddings,
significantly improves instance segmentation. Experiments on the NPM3D urban
mobile mapping dataset and the FOR-instance forest dataset demonstrate the
effectiveness and versatility of the proposed strategy.
</p></li>
</ul>
<h3>Title: DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms using Self-adversarial Learning. (arXiv:2307.02935v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02935">http://arxiv.org/abs/2307.02935</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02935] DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms using Self-adversarial Learning](http://arxiv.org/abs/2307.02935) #segmentation</code></li>
<li>Summary: <p>Asymmetry is a crucial characteristic of bilateral mammograms (Bi-MG) when
abnormalities are developing. It is widely utilized by radiologists for
diagnosis. The question of 'what the symmetrical Bi-MG would look like when the
asymmetrical abnormalities have been removed ?' has not yet received strong
attention in the development of algorithms on mammograms. Addressing this
question could provide valuable insights into mammographic anatomy and aid in
diagnostic interpretation. Hence, we propose a novel framework, DisAsymNet,
which utilizes asymmetrical abnormality transformer guided self-adversarial
learning for disentangling abnormalities and symmetric Bi-MG. At the same time,
our proposed method is partially guided by randomly synthesized abnormalities.
We conduct experiments on three public and one in-house dataset, and
demonstrate that our method outperforms existing methods in abnormality
classification, segmentation, and localization tasks. Additionally,
reconstructed normal mammograms can provide insights toward better
interpretable visual cues for clinical diagnosis. The code will be accessible
to the public.
</p></li>
</ul>
<h3>Title: Art Authentication with Vision Transformers. (arXiv:2307.03039v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03039">http://arxiv.org/abs/2307.03039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03039] Art Authentication with Vision Transformers](http://arxiv.org/abs/2307.03039) #segmentation</code></li>
<li>Summary: <p>In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
</p></li>
</ul>
<h2>3D point cloud</h2>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering. (arXiv:2307.02783v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02783">http://arxiv.org/abs/2307.02783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02783] UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering](http://arxiv.org/abs/2307.02783) #extraction</code></li>
<li>Summary: <p>In recent years, artificial intelligence has played an important role in
medicine and disease diagnosis, with many applications to be mentioned, one of
which is Medical Visual Question Answering (MedVQA). By combining computer
vision and natural language processing, MedVQA systems can assist experts in
extracting relevant information from medical image based on a given question
and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023
challenge carried out visual question answering task in the gastrointestinal
domain, which includes gastroscopy and colonoscopy images. Our team approached
Task 1 of the challenge by proposing a multimodal learning method with image
enhancement to improve the VQA performance on gastrointestinal images. The
multimodal architecture is set up with BERT encoder and different pre-trained
vision models based on convolutional neural network (CNN) and Transformer
architecture for features extraction from question and endoscopy image. The
result of this study highlights the dominance of Transformer-based vision
models over the CNNs and demonstrates the effectiveness of the image
enhancement process, with six out of the eight vision models achieving better
F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image
enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the
development test set, while also producing good result on the private test set
with accuracy of 82.01%.
</p></li>
</ul>
<h3>Title: FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02623">http://arxiv.org/abs/2307.02623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02623] FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout](http://arxiv.org/abs/2307.02623) #extraction</code></li>
<li>Summary: <p>Federated Learning (FL) allows machine learning models to train locally on
individual mobile devices, synchronizing model updates via a shared server.
This approach safeguards user privacy; however, it also generates a
heterogeneous training environment due to the varying performance capabilities
across devices. As a result, straggler devices with lower performance often
dictate the overall training time in FL. In this work, we aim to alleviate this
performance bottleneck due to stragglers by dynamically balancing the training
load across the system. We introduce Invariant Dropout, a method that extracts
a sub-model based on the weight update threshold, thereby minimizing potential
impacts on accuracy. Building on this dropout technique, we develop an adaptive
training framework, Federated Learning using Invariant Dropout (FLuID). FLuID
offers a lightweight sub-model extraction to regulate computational intensity,
thereby reducing the load on straggler devices without affecting model quality.
Our method leverages neuron updates from non-straggler devices to construct a
tailored sub-model for each straggler based on client performance profiling.
Furthermore, FLuID can dynamically adapt to changes in stragglers as runtime
conditions shift. We evaluate FLuID using five real-world mobile clients. The
evaluations show that Invariant Dropout maintains baseline model efficiency
while alleviating the performance bottleneck of stragglers through a dynamic,
runtime approach.
</p></li>
</ul>
<h3>Title: Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data. (arXiv:2307.02975v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.02975">http://arxiv.org/abs/2307.02975</a></li>
<li>Code URL: <a href="https://github.com/mattiacampana/transfer-learning-covid-19">https://github.com/mattiacampana/transfer-learning-covid-19</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.02975] Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data](http://arxiv.org/abs/2307.02975) #extraction</code></li>
<li>Summary: <p>Disease detection from smartphone data represents an open research challenge
in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are
an important case study in this area and their early detection is a potential
real instrument to counteract the pandemic situation. The efficacy of this
solution mainly depends on the performances of AI algorithms applied to the
collected data and their possible implementation directly on the users' mobile
devices. Considering these issues, and the limited amount of available data, in
this paper we present the experimental evaluation of 3 different deep learning
models, compared also with hand-crafted features, and of two main approaches of
transfer learning in the considered scenario: both feature extraction and
fine-tuning. Specifically, we considered VGGish, YAMNET, and
L\textsuperscript{3}-Net (including 12 different configurations) evaluated
through user-independent experiments on 4 different datasets (13,447 samples in
total). Results clearly show the advantages of L\textsuperscript{3}-Net in all
the experimental settings as it overcomes the other solutions by 12.3\% in
terms of Precision-Recall AUC as features extractor, and by 10\% when the model
is fine-tuned. Moreover, we note that to fine-tune only the fully-connected
layers of the pre-trained models generally leads to worse performances, with an
average drop of 6.6\% with respect to feature extraction. %highlighting the
need for further investigations. Finally, we evaluate the memory footprints of
the different models for their possible applications on commercial mobile
devices.
</p></li>
</ul>
<h2>Lidar</h2>
<h2>Infrastructure information models</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-09]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
