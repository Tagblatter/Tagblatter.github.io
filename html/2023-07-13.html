<h2>pointcloud</h2>
<h2>railway</h2>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: $\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model. (arXiv:2307.05617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05617">http://arxiv.org/abs/2307.05617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05617] $\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model](http://arxiv.org/abs/2307.05617) #segmentation</code></li>
<li>Summary: <p>Recently, large vision model, Segment Anything Model (SAM), has
revolutionized the computer vision field, especially for image segmentation.
SAM presented a new promptable segmentation paradigm that exhibit its
remarkable zero-shot generalization ability. An extensive researches have
explore the potential and limits of SAM in various downstream tasks. In this
study, we presents $\mathrm{SAM^{Med}}$, an enhanced framework for medical
image annotation that leverages the capabilities of SAM. $\mathrm{SAM^{Med}}$
framework consisted of two submodules, namely $\mathrm{SAM^{assist}}$ and
$\mathrm{SAM^{auto}}$. The $\mathrm{SAM^{assist}}$ demonstrates the
generalization ability of SAM to the downstream medical segmentation task using
the prompt-learning approach. Results show a significant improvement in
segmentation accuracy with only approximately 5 input points. The
$\mathrm{SAM^{auto}}$ model aims to accelerate the annotation process by
automatically generating input prompts. The proposed SAP-Net model achieves
superior segmentation performance with only five annotated slices, achieving an
average Dice coefficient of 0.80 and 0.82 for kidney and liver segmentation,
respectively. Overall, $\mathrm{SAM^{Med}}$ demonstrates promising results in
medical image annotation. These findings highlight the potential of leveraging
large-scale vision models in medical image annotation tasks.
</p></li>
</ul>
<h3>Title: HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding. (arXiv:2307.05721v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05721">http://arxiv.org/abs/2307.05721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05721] HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding](http://arxiv.org/abs/2307.05721) #segmentation</code></li>
<li>Summary: <p>Understanding comprehensive assembly knowledge from videos is critical for
futuristic ultra-intelligent industry. To enable technological breakthrough, we
present HA-ViD - the first human assembly video dataset that features
representative industrial assembly scenarios, natural procedural knowledge
acquisition process, and consistent human-robot shared annotations.
Specifically, HA-ViD captures diverse collaboration patterns of real-world
assembly, natural human behaviors and learning progression during assembly, and
granulate action annotations to subject, action verb, manipulated object,
target object, and tool. We provide 3222 multi-view, multi-modality videos
(each video contains one assembly task), 1.5M frames, 96K temporal labels and
2M spatial labels. We benchmark four foundational video understanding tasks:
action recognition, action segmentation, object detection and multi-object
tracking. Importantly, we analyze their performance for comprehending knowledge
in assembly progress, process efficiency, task collaboration, skill parameters
and human intention. Details of HA-ViD is available at:
https://iai-hrc.github.io/ha-vid.
</p></li>
</ul>
<h3>Title: OG: Equip vision occupancy with instance segmentation and visual grounding. (arXiv:2307.05873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05873">http://arxiv.org/abs/2307.05873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05873] OG: Equip vision occupancy with instance segmentation and visual grounding](http://arxiv.org/abs/2307.05873) #segmentation</code></li>
<li>Summary: <p>Occupancy prediction tasks focus on the inference of both geometry and
semantic labels for each voxel, which is an important perception mission.
However, it is still a semantic segmentation task without distinguishing
various instances. Further, although some existing works, such as
Open-Vocabulary Occupancy (OVO), have already solved the problem of open
vocabulary detection, visual grounding in occupancy has not been solved to the
best of our knowledge. To tackle the above two limitations, this paper proposes
Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance
segmentation ability and could operate visual grounding in a voxel manner with
the help of grounded-SAM. Keys to our approach are (1) affinity field
prediction for instance clustering and (2) association strategy for aligning 2D
instance masks and 3D occupancy instances. Extensive experiments have been
conducted whose visualization results and analysis are shown below. Our code
will be publicly released soon.
</p></li>
</ul>
<h3>Title: Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation. (arXiv:2307.05898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05898">http://arxiv.org/abs/2307.05898</a></li>
<li>Code URL: <a href="https://github.com/beileicui/ms-tfal">https://github.com/beileicui/ms-tfal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05898] Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation](http://arxiv.org/abs/2307.05898) #segmentation</code></li>
<li>Summary: <p>Noisy label problems are inevitably in existence within medical image
segmentation causing severe performance degradation. Previous segmentation
methods for noisy label problems only utilize a single image while the
potential of leveraging the correlation between images has been overlooked.
Especially for video segmentation, adjacent frames contain rich contextual
information beneficial in cognizing noisy labels. Based on two insights, we
propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to
resolve noisy-labeled medical video segmentation issues. First, we argue the
sequential prior of videos is an effective reference, i.e., pixel-level
features from adjacent frames are close in distance for the same class and far
in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is
devised to indicate possible noisy labels by evaluating the affinity between
pixels in two adjacent frames. We also notice that the noise distribution
exhibits considerable variations across video, image, and pixel levels. In this
way, we introduce Multi-Scale Supervision (MSS) to supervise the network from
three different perspectives by re-weighting and refining the samples. This
design enables the network to concentrate on clean samples in a coarse-to-fine
manner. Experiments with both synthetic and real-world label noise demonstrate
that our method outperforms recent state-of-the-art robust segmentation
approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.
</p></li>
</ul>
<h3>Title: RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation. (arXiv:2307.06099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06099">http://arxiv.org/abs/2307.06099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06099] RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation](http://arxiv.org/abs/2307.06099) #segmentation</code></li>
<li>Summary: <p>Glass-like objects are widespread in daily life but remain intractable to be
segmented for most existing methods. The transparent property makes it
difficult to be distinguished from background, while the tiny separation
boundary further impedes the acquisition of their exact contour. In this paper,
by revealing the key co-evolution demand of semantic and boundary learning, we
propose a Selective Mutual Evolution (SME) module to enable the reciprocal
feature learning between them. Then to exploit the global shape context, we
propose a Structurally Attentive Refinement (SAR) module to conduct a
fine-grained feature refinement for those ambiguous points around the boundary.
Finally, to further utilize the multi-scale representation, we integrate the
above two modules into a cascaded structure and then introduce a Reciprocal
Feature Evolution Network (RFENet) for effective glass-like object
segmentation. Extensive experiments demonstrate that our RFENet achieves
state-of-the-art performance on three popular public datasets.
</p></li>
</ul>
<h3>Title: Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06304">http://arxiv.org/abs/2307.06304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06304] Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](http://arxiv.org/abs/2307.06304) #segmentation</code></li>
<li>Summary: <p>The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
</p></li>
</ul>
<h3>Title: Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation. (arXiv:2307.06312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06312">http://arxiv.org/abs/2307.06312</a></li>
<li>Code URL: <a href="https://github.com/herschel555/caml">https://github.com/herschel555/caml</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06312] Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2307.06312) #segmentation</code></li>
<li>Summary: <p>Semi-supervised learning has become increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods only focus on extracting information from unlabeled data,
disregarding the potential of labeled data to further improve the performance
of the model. In this paper, we propose a novel Correlation Aware Mutual
Learning (CAML) framework that leverages labeled data to guide the extraction
of information from unlabeled data. Our approach is based on a mutual learning
strategy that incorporates two modules: the Cross-sample Mutual Attention
Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module
establishes dense cross-sample correlations among a group of samples, enabling
the transfer of label prior knowledge to unlabeled data. The OCC module
constructs omni-correlations between the unlabeled and labeled datasets and
regularizes dual models by constraining the omni-correlation matrix of each
sub-model to be consistent. Experiments on the Atrial Segmentation Challenge
dataset demonstrate that our proposed approach outperforms state-of-the-art
methods, highlighting the effectiveness of our framework in medical image
segmentation tasks. The codes, pre-trained weights, and data are publicly
available.
</p></li>
</ul>
<h3>Title: Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of "Never Enough Training Data". (arXiv:2307.06322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06322">http://arxiv.org/abs/2307.06322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06322] Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of "Never Enough Training Data"](http://arxiv.org/abs/2307.06322) #segmentation</code></li>
<li>Summary: <p>Crystalline defects, such as line-like dislocations, play an important role
for the performance and reliability of many metallic devices. Their interaction
and evolution still poses a multitude of open questions to materials science
and materials physics. In-situ TEM experiments can provide important insights
into how dislocations behave and move. During such experiments, the dislocation
microstructure is captured in form of videos. The analysis of individual video
frames can provide useful insights but is limited by the capabilities of
automated identification, digitization, and quantitative extraction of the
dislocations as curved objects. The vast amount of data also makes manual
annotation very time consuming, thereby limiting the use of Deep
Learning-based, automated image analysis and segmentation of the dislocation
microstructure. In this work, a parametric model for generating synthetic
training data for segmentation of dislocations is developed. Even though domain
scientists might dismiss synthetic training images sometimes as too artificial,
our findings show that they can result in superior performance, particularly
regarding the generalizing of the Deep Learning models with respect to
different microstructures and imaging conditions. Additionally, we propose an
enhanced deep learning method optimized for segmenting overlapping or
intersecting dislocation lines. Upon testing this framework on four distinct
real datasets, we find that our synthetic training data are able to yield
high-quality results also on real images-even more so if fine-tune on a few
real images was done.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: Line Art Colorization of Fakemon using Generative Adversarial Neural Networks. (arXiv:2307.05760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05760">http://arxiv.org/abs/2307.05760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05760] Line Art Colorization of Fakemon using Generative Adversarial Neural Networks](http://arxiv.org/abs/2307.05760) #extraction</code></li>
<li>Summary: <p>This work proposes a complete methodology to colorize images of Fakemon,
anime-style monster-like creatures. In addition, we propose algorithms to
extract the line art from colorized images as well as to extract color hints.
Our work is the first in the literature to use automatic color hint extraction,
to train the networks specifically with anime-styled creatures and to combine
the Pix2Pix and CycleGAN approaches, two different generative adversarial
networks that create a single final result. Visual results of the colorizations
are feasible but there is still room for improvement.
</p></li>
</ul>
<h2>lidar</h2>
<h3>Title: The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset. (arXiv:2307.06165v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06165">http://arxiv.org/abs/2307.06165</a></li>
<li>Code URL: <a href="https://github.com/kav-institute/imptc-dataset">https://github.com/kav-institute/imptc-dataset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06165] The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset](http://arxiv.org/abs/2307.06165) #lidar</code></li>
<li>Summary: <p>Inner-city intersections are among the most critical traffic areas for injury
and fatal accidents. Automated vehicles struggle with the complex and hectic
everyday life within those areas. Sensor-equipped smart infrastructures, which
can cooperate with vehicles, can benefit automated traffic by extending the
perception capabilities of drivers and vehicle perception systems.
Additionally, they offer the opportunity to gather reproducible and precise
data of a holistic scene understanding, including context information as a
basis for training algorithms for various applications in automated traffic.
Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context
Dataset (IMPTC). We use an intelligent public inner-city intersection in
Germany with visual sensor technology. A multi-view camera and LiDAR system
perceives traffic situations and road users' behavior. Additional sensors
monitor contextual information like weather, lighting, and traffic light signal
status. The data acquisition system focuses on Vulnerable Road Users (VRUs) and
multi-agent interaction. The resulting dataset consists of eight hours of
measurement data. It contains over 2,500 VRU trajectories, including
pedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and
over 20,000 vehicle trajectories at different day times, weather conditions,
and seasons. In addition, to enable the entire stack of research capabilities,
the dataset includes all data, starting from the sensor-, calibration- and
detection data until trajectory and context data. The dataset is continuously
expanded and is available online for non-commercial research at
https://github.com/kav-institute/imptc-dataset.
</p></li>
</ul>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-13]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
