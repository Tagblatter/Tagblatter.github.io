<h2>pointcloud</h2>
<h2>railway</h2>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images. (arXiv:2308.05137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05137">http://arxiv.org/abs/2308.05137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05137] Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images](http://arxiv.org/abs/2308.05137) #segmentation</code></li>
<li>Summary: <p>Weakly supervised methods, such as class activation maps (CAM) based, have
been applied to achieve bleeding segmentation with low annotation efforts in
Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be
extremely noisy, and there is an irreparable gap between CAM labels and ground
truths for medical images. This paper proposes a new Discrepancy-basEd Active
Learning (DEAL) approach to bridge the gap between CAMs and ground truths with
a few annotations. Specifically, to liberate labor, we design a novel
discrepancy decoder model and a CAMPUS (CAM, Pseudo-label and groUnd-truth
Selection) criterion to replace the noisy CAMs with accurate model predictions
and a few human labels. The discrepancy decoder model is trained with a unique
scheme to generate standard, coarse and fine predictions. And the CAMPUS
criterion is proposed to predict the gaps between CAMs and ground truths based
on model divergence and CAM divergence. We evaluate our method on the WCE
dataset and results show that our method outperforms the state-of-the-art
active learning methods and reaches comparable performance to those trained
with full annotated datasets with only 10% of the training data labeled.
</p></li>
</ul>
<h3>Title: A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision. (arXiv:2308.05168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05168">http://arxiv.org/abs/2308.05168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05168] A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision](http://arxiv.org/abs/2308.05168) #segmentation</code></li>
<li>Summary: <p>Existing model evaluation tools mainly focus on evaluating classification
models, leaving a gap in evaluating more complex models, such as object
detection. In this paper, we develop an open-source visual analysis tool,
Uni-Evaluator, to support a unified model evaluation for classification, object
detection, and instance segmentation in computer vision. The key idea behind
our method is to formulate both discrete and continuous predictions in
different tasks as unified probability distributions. Based on these
distributions, we develop 1) a matrix-based visualization to provide an
overview of model performance; 2) a table visualization to identify the
problematic data subsets where the model performs poorly; 3) a grid
visualization to display the samples of interest. These visualizations work
together to facilitate the model evaluation from a global overview to
individual samples. Two case studies demonstrate the effectiveness of
Uni-Evaluator in evaluating model performance and making informed improvements.
</p></li>
</ul>
<h3>Title: SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05232">http://arxiv.org/abs/2308.05232</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05232] SegMatch: A semi-supervised learning method for surgical instrument segmentation](http://arxiv.org/abs/2308.05232) #segmentation</code></li>
<li>Summary: <p>Surgical instrument segmentation is recognised as a key enabler to provide
advanced surgical assistance and improve computer assisted interventions. In
this work, we propose SegMatch, a semi supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are weakly augmented and fed into the segmentation model to generate a
pseudo-label to enforce the unsupervised loss against the output of the model
for the adversarial augmented image on the pixels with a high confidence score.
Our adaptation for segmentation tasks includes carefully considering the
equivariance and invariance properties of the augmentation functions we rely
on. To increase the relevance of our augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation
Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate
that adding unlabelled data for training purposes allows us to surpass the
performance of fully supervised approaches which are limited by the
availability of training data in these challenges. SegMatch also outperforms a
range of state-of-the-art semi-supervised learning semantic segmentation models
in different labelled to unlabelled data ratios.
</p></li>
</ul>
<h3>Title: Deep Semantic Graph Matching for Large-scale Outdoor Point Clouds Registration. (arXiv:2308.05314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05314">http://arxiv.org/abs/2308.05314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05314] Deep Semantic Graph Matching for Large-scale Outdoor Point Clouds Registration](http://arxiv.org/abs/2308.05314) #segmentation</code></li>
<li>Summary: <p>The current point cloud registration methods are mainly based on geometric
information and usually ignore the semantic information in the point clouds. In
this paper, we treat the point cloud registration problem as semantic instance
matching and registration task, and propose a deep semantic graph matching
method for large-scale outdoor point cloud registration. Firstly, the semantic
category labels of 3D point clouds are obtained by utilizing large-scale point
cloud semantic segmentation network. The adjacent points with the same category
labels are then clustered together by using Euclidean clustering algorithm to
obtain the semantic instances. Secondly, the semantic adjacency graph is
constructed based on the spatial adjacency relation of semantic instances.
Three kinds of high-dimensional features including geometric shape features,
semantic categorical features and spatial distribution features are learned
through graph convolutional network, and enhanced based on attention mechanism.
Thirdly, the semantic instance matching problem is modeled as an optimal
transport problem, and solved through an optimal matching layer. Finally,
according to the matched semantic instances, the geometric transformation
matrix between two point clouds is first obtained by SVD algorithm and then
refined by ICP algorithm. The experiments are cconducted on the KITTI Odometry
dataset, and the average relative translation error and average relative
rotation error of the proposed method are 6.6cm and 0.229{\deg} respectively.
</p></li>
</ul>
<h3>Title: Fine-grained building roof instance segmentation based on domain adapted pretraining and composite dual-backbone. (arXiv:2308.05358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05358">http://arxiv.org/abs/2308.05358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05358] Fine-grained building roof instance segmentation based on domain adapted pretraining and composite dual-backbone](http://arxiv.org/abs/2308.05358) #segmentation</code></li>
<li>Summary: <p>The diversity of building architecture styles of global cities situated on
various landforms, the degraded optical imagery affected by clouds and shadows,
and the significant inter-class imbalance of roof types pose challenges for
designing a robust and accurate building roof instance segmentor. To address
these issues, we propose an effective framework to fulfill semantic
interpretation of individual buildings with high-resolution optical satellite
imagery. Specifically, the leveraged domain adapted pretraining strategy and
composite dual-backbone greatly facilitates the discriminative feature
learning. Moreover, new data augmentation pipeline, stochastic weight averaging
(SWA) training and instance segmentation based model ensemble in testing are
utilized to acquire additional performance boost. Experiment results show that
our approach ranks in the first place of the 2023 IEEE GRSS Data Fusion Contest
(DFC) Track 1 test phase ($mAP_{50}$:50.6\%). Note-worthily, we have also
explored the potential of multimodal data fusion with both optical satellite
imagery and SAR data.
</p></li>
</ul>
<h3>Title: Pseudo-label Alignment for Semi-supervised Instance Segmentation. (arXiv:2308.05359v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05359">http://arxiv.org/abs/2308.05359</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05359] Pseudo-label Alignment for Semi-supervised Instance Segmentation](http://arxiv.org/abs/2308.05359) #segmentation</code></li>
<li>Summary: <p>Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.
</p></li>
</ul>
<h3>Title: Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection. (arXiv:2308.05426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05426">http://arxiv.org/abs/2308.05426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05426] Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection](http://arxiv.org/abs/2308.05426) #segmentation</code></li>
<li>Summary: <p>Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.
</p></li>
</ul>
<h3>Title: Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation. (arXiv:2308.05493v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05493">http://arxiv.org/abs/2308.05493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05493] Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation](http://arxiv.org/abs/2308.05493) #segmentation</code></li>
<li>Summary: <p>Endeavors have been recently made to transfer knowledge from the labeled
pinhole image domain to the unlabeled panoramic image domain via Unsupervised
Domain Adaptation (UDA). The aim is to tackle the domain gaps caused by the
style disparities and distortion problem from the non-uniformly distributed
pixels of equirectangular projection (ERP). Previous works typically focus on
transferring knowledge based on geometric priors with specially designed
multi-branch network architectures. As a result, considerable computational
costs are induced, and meanwhile, their generalization abilities are profoundly
hindered by the variation of distortion among pixels. In this paper, we find
that the pixels' neighborhood regions of the ERP indeed introduce less
distortion. Intuitively, we propose a novel UDA framework that can effectively
address the distortion problems for panoramic semantic segmentation. In
comparison, our method is simpler, easier to implement, and more
computationally efficient. Specifically, we propose distortion-aware attention
(DA) capturing the neighboring pixel distribution without using any geometric
constraints. Moreover, we propose a class-wise feature aggregation (CFA) module
to iteratively update the feature representations with a memory bank. As such,
the feature similarity between two domains can be consistently optimized.
Extensive experiments show that our method achieves new state-of-the-art
performance while remarkably reducing 80% parameters.
</p></li>
</ul>
<h3>Title: Category Feature Transformer for Semantic Segmentation. (arXiv:2308.05581v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05581">http://arxiv.org/abs/2308.05581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05581] Category Feature Transformer for Semantic Segmentation](http://arxiv.org/abs/2308.05581) #segmentation</code></li>
<li>Summary: <p>Aggregation of multi-stage features has been revealed to play a significant
role in semantic segmentation. Unlike previous methods employing point-wise
summation or concatenation for feature aggregation, this study proposes the
Category Feature Transformer (CFT) that explores the flow of category embedding
and transformation among multi-stage features through the prevalent multi-head
attention mechanism. CFT learns unified feature embeddings for individual
semantic categories from high-level features during each aggregation process
and dynamically broadcasts them to high-resolution features. Integrating the
proposed CFT into a typical feature pyramid structure exhibits superior
performance over a broad range of backbone networks. We conduct extensive
experiments on popular semantic segmentation benchmarks. Specifically, the
proposed CFT obtains a compelling 55.1% mIoU with greatly reduced model
parameters and computations on the challenging ADE20K dataset.
</p></li>
</ul>
<h3>Title: Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network. (arXiv:2308.05605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05605">http://arxiv.org/abs/2308.05605</a></li>
<li>Code URL: <a href="https://github.com/wencheng256/daccn">https://github.com/wencheng256/daccn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05605] Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network](http://arxiv.org/abs/2308.05605) #segmentation</code></li>
<li>Summary: <p>Monocular depth estimation is known as an ill-posed task in which objects in
a 2D image usually do not contain sufficient information to predict their
depth. Thus, it acts differently from other tasks (e.g., classification and
segmentation) in many ways. In this paper, we find that self-supervised
monocular depth estimation shows a direction sensitivity and environmental
dependency in the feature representation. But the current backbones borrowed
from other tasks pay less attention to handling different types of
environmental information, limiting the overall depth accuracy. To bridge this
gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN),
which improves the depth feature representation in two aspects. First, we
propose a direction-aware module, which can learn to adjust the feature
extraction in each direction, facilitating the encoding of different types of
information. Secondly, we design a new cumulative convolution to improve the
efficiency for aggregating important environmental information. Experiments
show that our method achieves significant improvements on three widely used
benchmarks, KITTI, Cityscapes, and Make3D, setting a new state-of-the-art
performance on the popular benchmarks with all three types of self-supervision.
</p></li>
</ul>
<h3>Title: Masked Diffusion as Self-supervised Representation Learner. (arXiv:2308.05695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05695">http://arxiv.org/abs/2308.05695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05695] Masked Diffusion as Self-supervised Representation Learner](http://arxiv.org/abs/2308.05695) #segmentation</code></li>
<li>Summary: <p>Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and been used as strong pixel-level
representation learners. This paper decomposes the interrelation between the
generative capability and representation learning ability inherent in diffusion
models. We present masked diffusion model (MDM), a scalable self-supervised
representation learner that substitutes the conventional additive Gaussian
noise of traditional diffusion with a masking mechanism. Our proposed approach
convincingly surpasses prior benchmarks, demonstrating remarkable advancements
in both medical and natural image semantic segmentation tasks, particularly
within the context of few-shot scenario.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h3>Title: Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05525">http://arxiv.org/abs/2308.05525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05525] Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI](http://arxiv.org/abs/2308.05525) #3d point cloud</code></li>
<li>Summary: <p>The ability to cope accurately and fast with Out-Of-Distribution (OOD)
samples is crucial in real-world safety demanding applications. In this work we
first study the interplay between critical points of 3D point clouds and OOD
samples. Our findings are that common corruptions and outliers are often
interpreted as critical points. We generalize the notion of critical points
into importance measures. We show that training a classification network based
only on less important points dramatically improves robustness, at a cost of
minor performance loss on the clean set. We observe that normalized entropy is
highly informative for corruption analysis. An adaptive threshold based on
normalized entropy is suggested for selecting the set of uncritical points. Our
proposed importance measure is extremely fast to compute. We show it can be
used for a variety of applications, such as Explainable AI (XAI), Outlier
Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense.
We reach SOTA results on the two latter tasks.
</p></li>
</ul>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping. (arXiv:2308.05235v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05235">http://arxiv.org/abs/2308.05235</a></li>
<li>Code URL: <a href="https://github.com/aj1365/sgumlp">https://github.com/aj1365/sgumlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05235] Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping](http://arxiv.org/abs/2308.05235) #extraction</code></li>
<li>Summary: <p>Convolutional Neural Networks (CNNs) are models that are utilized extensively
for the hierarchical extraction of features. Vision transformers (ViTs),
through the use of a self-attention mechanism, have recently achieved superior
modeling of global contextual information compared to CNNs. However, to realize
their image classification strength, ViTs require substantial training
datasets. Where the available training data are limited, current advanced
multi-layer perceptrons (MLPs) can provide viable alternatives to both deep
CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm
that effectively uses both MLPs and spatial gating units (SGUs) for precise
land use land cover (LULC) mapping. Results illustrated the superiority of the
developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based
models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The
proposed SGU-MLP algorithm was tested through three experiments in Houston,
USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model
was found to consistently outperform the benchmark CNN and CNN-ViT-based
algorithms. For example, for the Houston experiment, SGU-MLP significantly
outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by
approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average
accuracy. The code will be made publicly available at
https://github.com/aj1365/SGUMLP
</p></li>
</ul>
<h3>Title: Flexible Isosurface Extraction for Gradient-Based Mesh Optimization. (arXiv:2308.05371v1 [cs.GR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05371">http://arxiv.org/abs/2308.05371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05371] Flexible Isosurface Extraction for Gradient-Based Mesh Optimization](http://arxiv.org/abs/2308.05371) #extraction</code></li>
<li>Summary: <p>This work considers gradient-based mesh optimization, where we iteratively
optimize for a 3D surface mesh by representing it as the isosurface of a scalar
field, an increasingly common paradigm in applications including
photogrammetry, generative modeling, and inverse physics. Existing
implementations adapt classic isosurface extraction algorithms like Marching
Cubes or Dual Contouring; these techniques were designed to extract meshes from
fixed, known fields, and in the optimization setting they lack the degrees of
freedom to represent high-quality feature-preserving meshes, or suffer from
numerical instabilities. We introduce FlexiCubes, an isosurface representation
specifically designed for optimizing an unknown mesh with respect to geometric,
visual, or even physical objectives. Our main insight is to introduce
additional carefully-chosen parameters into the representation, which allow
local flexible adjustments to the extracted mesh geometry and connectivity.
These parameters are updated along with the underlying scalar field via
automatic differentiation when optimizing for a downstream task. We base our
extraction scheme on Dual Marching Cubes for improved topological properties,
and present extensions to optionally generate tetrahedral and
hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on
both synthetic benchmarks and real-world applications, showing that it offers
significant improvements in mesh quality and geometric fidelity.
</p></li>
</ul>
<h3>Title: HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation. (arXiv:2308.05387v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05387">http://arxiv.org/abs/2308.05387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05387] HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation](http://arxiv.org/abs/2308.05387) #extraction</code></li>
<li>Summary: <p>Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.
</p></li>
</ul>
<h3>Title: Learning Gabor Texture Features for Fine-Grained Recognition. (arXiv:2308.05396v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05396">http://arxiv.org/abs/2308.05396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05396] Learning Gabor Texture Features for Fine-Grained Recognition](http://arxiv.org/abs/2308.05396) #extraction</code></li>
<li>Summary: <p>Extracting and using class-discriminative features is critical for
fine-grained recognition. Existing works have demonstrated the possibility of
applying deep CNNs to exploit features that distinguish similar classes.
However, CNNs suffer from problems including frequency bias and loss of
detailed local information, which restricts the performance of recognizing
fine-grained categories. To address the challenge, we propose a novel texture
branch as complimentary to the CNN branch for feature extraction. We
innovatively utilize Gabor filters as a powerful extractor to exploit texture
features, motivated by the capability of Gabor filters in effectively capturing
multi-frequency features and detailed local information. We implement several
designs to enhance the effectiveness of Gabor filters, including imposing
constraints on parameter values and developing a learning method to determine
the optimal parameters. Moreover, we introduce a statistical feature extractor
to utilize informative statistical information from the signals captured by
Gabor filters, and a gate selection mechanism to enable efficient computation
by only considering qualified regions as input for texture extraction. Through
the integration of features from the Gabor-filter-based texture branch and
CNN-based semantic branch, we achieve comprehensive information extraction. We
demonstrate the efficacy of our method on multiple datasets, including
CUB-200-2011, NA-bird, Stanford Dogs, and GTOS-mobile. State-of-the-art
performance is achieved using our approach.
</p></li>
</ul>
<h3>Title: Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation. (arXiv:2308.05438v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05438">http://arxiv.org/abs/2308.05438</a></li>
<li>Code URL: <a href="https://github.com/junzastar/dftr_voting">https://github.com/junzastar/dftr_voting</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05438] Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation](http://arxiv.org/abs/2308.05438) #extraction</code></li>
<li>Summary: <p>One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.
</p></li>
</ul>
<h3>Title: A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement. (arXiv:2308.05447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05447">http://arxiv.org/abs/2308.05447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05447] A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement](http://arxiv.org/abs/2308.05447) #extraction</code></li>
<li>Summary: <p>Underwater images often suffer from color distortion and low contrast
resulting in various image types, due to the scattering and absorption of light
by water. While it is difficult to obtain high-quality paired training samples
with a generalized model. To tackle these challenges, we design a Generalized
Underwater image enhancement method via a Physical-knowledge-guided Dynamic
Model (short for GUPDM), consisting of three parts: Atmosphere-based Dynamic
Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based
Multi-scale Structure (PMS). In particular, to cover complex underwater scenes,
this study changes the global atmosphere light and the transmission to simulate
various underwater image types (e.g., the underwater image color ranging from
yellow to blue) through the formation model. We then design ADS and TDS that
use dynamic convolutions to adaptively extract prior information from
underwater images and generate parameters for PMS. These two modules enable the
network to select appropriate parameters for various water types adaptively.
Besides, the multi-scale feature extraction module in PMS uses convolution
blocks with different kernel sizes and obtains weights for each feature map via
channel attention block and fuses them to boost the receptive field of the
network. The source code will be available at
\href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM}.
</p></li>
</ul>
<h2>lidar</h2>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-13]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
