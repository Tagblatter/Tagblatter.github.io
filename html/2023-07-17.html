<h2>pointcloud</h2>
<h2>railway</h2>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07044">http://arxiv.org/abs/2307.07044</a></li>
<li>Code URL: <a href="https://github.com/neel-dey/anystar">https://github.com/neel-dey/anystar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07044] AnyStar: Domain randomized universal star-convex 3D instance segmentation](http://arxiv.org/abs/2307.07044) #segmentation</code></li>
<li>Summary: <p>Star-convex shapes arise across bio-microscopy and radiology in the form of
nuclei, nodules, metastases, and other units. Existing instance segmentation
networks for such structures train on densely labeled instances for each
dataset, which requires substantial and often impractical manual annotation
effort. Further, significant reengineering or finetuning is needed when
presented with new datasets and imaging modalities due to changes in contrast,
shape, orientation, resolution, and density. We present AnyStar, a
domain-randomized generative model that simulates synthetic training data of
blob-like objects with randomized appearance, environments, and imaging physics
to train general-purpose star-convex instance segmentation networks. As a
result, networks trained using our generative model do not require annotated
images from unseen datasets. A single network trained on our synthesized data
accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence
microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM,
and placental cotyledons in human fetal MRI, all without any retraining,
finetuning, transfer learning, or domain adaptation. Code is available at
https://github.com/neel-dey/AnyStar.
</p></li>
</ul>
<h3>Title: Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar. (arXiv:2307.07102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07102">http://arxiv.org/abs/2307.07102</a></li>
<li>Code URL: <a href="https://github.com/GuanRunwei/Achelous">https://github.com/GuanRunwei/Achelous</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07102] Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar](http://arxiv.org/abs/2307.07102) #segmentation</code></li>
<li>Summary: <p>Current perception models for different tasks usually exist in modular forms
on Unmanned Surface Vehicles (USVs), which infer extremely slowly in parallel
on edge devices, causing the asynchrony between perception results and USV
position, and leading to error decisions of autonomous navigation. Compared
with Unmanned Ground Vehicles (UGVs), the robust perception of USVs develops
relatively slowly. Moreover, most current multi-task perception models are huge
in parameters, slow in inference and not scalable. Oriented on this, we propose
Achelous, a low-cost and fast unified panoptic perception framework for
water-surface perception based on the fusion of a monocular camera and 4D
mmWave radar. Achelous can simultaneously perform five tasks, detection and
segmentation of visual targets, drivable-area segmentation, waterline
segmentation and radar point cloud segmentation. Besides, models in Achelous
family, with less than around 5 million parameters, achieve about 18 FPS on an
NVIDIA Jetson AGX Xavier, 11 FPS faster than HybridNets, and exceed YOLOX-Tiny
and Segformer-B0 on our collected dataset about 5 mAP$_{\text{50-95}}$ and 0.7
mIoU, especially under situations of adverse weather, dark environments and
camera failure. To our knowledge, Achelous is the first comprehensive panoptic
perception framework combining vision-level and point-cloud-level tasks for
water-surface perception. To promote the development of the intelligent
transportation community, we release our codes in
\url{https://github.com/GuanRunwei/Achelous}.
</p></li>
</ul>
<h3>Title: Improved Flood Insights: Diffusion-Based SAR to EO Image Translation. (arXiv:2307.07123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07123">http://arxiv.org/abs/2307.07123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07123] Improved Flood Insights: Diffusion-Based SAR to EO Image Translation](http://arxiv.org/abs/2307.07123) #segmentation</code></li>
<li>Summary: <p>Driven by rapid climate change, the frequency and intensity of flood events
are increasing. Electro-Optical (EO) satellite imagery is commonly utilized for
rapid response. However, its utilities in flood situations are hampered by
issues such as cloud cover and limitations during nighttime, making accurate
assessment of damage challenging. Several alternative flood detection
techniques utilizing Synthetic Aperture Radar (SAR) data have been proposed.
Despite the advantages of SAR over EO in the aforementioned situations, SAR
presents a distinct drawback: human analysts often struggle with data
interpretation. To tackle this issue, this paper introduces a novel framework,
Diffusion-Based SAR to EO Image Translation (DSE). The DSE framework converts
SAR images into EO images, thereby enhancing the interpretability of flood
insights for humans. Experimental results on the Sen1Floods11 and SEN12-FLOOD
datasets confirm that the DSE framework not only delivers enhanced visual
information but also improves performance across all tested flood segmentation
baselines.
</p></li>
</ul>
<h3>Title: Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation. (arXiv:2307.07168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07168">http://arxiv.org/abs/2307.07168</a></li>
<li>Code URL: <a href="https://github.com/deepmicroscopy/adaptiveregionselection">https://github.com/deepmicroscopy/adaptiveregionselection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07168] Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation](http://arxiv.org/abs/2307.07168) #segmentation</code></li>
<li>Summary: <p>The process of annotating histological gigapixel-sized whole slide images
(WSIs) at the pixel level for the purpose of training a supervised segmentation
model is time-consuming. Region-based active learning (AL) involves training
the model on a limited number of annotated image regions instead of requesting
annotations of the entire images. These annotation regions are iteratively
selected, with the goal of optimizing model performance while minimizing the
annotated area. The standard method for region selection evaluates the
informativeness of all square regions of a specified size and then selects a
specific quantity of the most informative regions. We find that the efficiency
of this method highly depends on the choice of AL step size (i.e., the
combination of region size and the number of selected regions per WSI), and a
suboptimal AL step size can result in redundant annotation requests or inflated
computation costs. This paper introduces a novel technique for selecting
annotation regions adaptively, mitigating the reliance on this AL
hyperparameter. Specifically, we dynamically determine each region by first
identifying an informative area and then detecting its optimal bounding box, as
opposed to selecting regions of a uniform predefined shape and size as in the
standard method. We evaluate our method using the task of breast cancer
metastases segmentation on the public CAMELYON16 dataset and show that it
consistently achieves higher sampling efficiency than the standard method
across various AL step sizes. With only 2.6\% of tissue area annotated, we
achieve full annotation performance and thereby substantially reduce the costs
of annotating a WSI dataset. The source code is available at
https://github.com/DeepMicroscopy/AdaptiveRegionSelection.
</p></li>
</ul>
<h3>Title: Challenge Results Are Not Reproducible. (arXiv:2307.07226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07226">http://arxiv.org/abs/2307.07226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07226] Challenge Results Are Not Reproducible](http://arxiv.org/abs/2307.07226) #segmentation</code></li>
<li>Summary: <p>While clinical trials are the state-of-the-art methods to assess the effect
of new medication in a comparative manner, benchmarking in the field of medical
image analysis is performed by so-called challenges. Recently, comprehensive
analysis of multiple biomedical image analysis challenges revealed large
discrepancies between the impact of challenges and quality control of the
design and reporting standard. This work aims to follow up on these results and
attempts to address the specific question of the reproducibility of the
participants methods. In an effort to determine whether alternative
interpretations of the method description may change the challenge ranking, we
reproduced the algorithms submitted to the 2019 Robust Medical Image
Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially
between the original challenge and reimplementation, indicating that challenge
rankings may not be sufficiently reproducible.
</p></li>
</ul>
<h3>Title: FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation. (arXiv:2307.07245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07245">http://arxiv.org/abs/2307.07245</a></li>
<li>Code URL: <a href="https://github.com/ty-shi/freecos">https://github.com/ty-shi/freecos</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07245] FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation](http://arxiv.org/abs/2307.07245) #segmentation</code></li>
<li>Summary: <p>Curvilinear object segmentation is critical for many applications. However,
manually annotating curvilinear objects is very time-consuming and error-prone,
yielding insufficiently available annotated datasets for existing supervised
methods and domain adaptation methods. This paper proposes a self-supervised
curvilinear object segmentation method that learns robust and distinctive
features from fractals and unlabeled images (FreeCOS). The key contributions
include a novel Fractal-FDA synthesis (FFS) module and a geometric information
alignment (GIA) approach. FFS generates curvilinear structures based on the
parametric Fractal L-system and integrates the generated structures into
unlabeled images to obtain synthetic training images via Fourier Domain
Adaptation. GIA reduces the intensity differences between the synthetic and
unlabeled images by comparing the intensity order of a given pixel to the
values of its nearby neighbors. Such image alignment can explicitly remove the
dependency on absolute intensity values and enhance the inherent geometric
characteristics which are common in both synthetic and real images. In
addition, GIA aligns features of synthetic and real images via the prediction
space adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL).
Extensive experimental results on four public datasets, i.e., XCAD, DRIVE,
STARE and CrackTree demonstrate that our method outperforms the
state-of-the-art unsupervised methods, self-supervised methods and traditional
methods by a large margin. The source code of this work is available at
https://github.com/TY-Shi/FreeCOS.
</p></li>
</ul>
<h3>Title: Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07246">http://arxiv.org/abs/2307.07246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07246] Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training](http://arxiv.org/abs/2307.07246) #segmentation</code></li>
<li>Summary: <p>The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.
</p></li>
</ul>
<h3>Title: HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07313">http://arxiv.org/abs/2307.07313</a></li>
<li>Code URL: <a href="https://github.com/janegerken/heal-swin">https://github.com/janegerken/heal-swin</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07313] HEAL-SWIN: A Vision Transformer On The Sphere](http://arxiv.org/abs/2307.07313) #segmentation</code></li>
<li>Summary: <p>High-resolution wide-angle fisheye images are becoming more and more
important for robotics applications such as autonomous driving. However, using
ordinary convolutional neural networks or vision transformers on this data is
problematic due to projection and distortion losses introduced when projecting
to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,
which combines the highly uniform Hierarchical Equal Area iso-Latitude
Pixelation (HEALPix) grid used in astrophysics and cosmology with the
Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and
flexible model capable of training on high-resolution, distortion-free
spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used
to perform the patching and windowing operations of the SWIN transformer,
resulting in a one-dimensional representation of the spherical data with
minimal computational overhead. We demonstrate the superior performance of our
model for semantic segmentation and depth regression tasks on both synthetic
and real automotive datasets. Our code is available at
https://github.com/JanEGerken/HEAL-SWIN.
</p></li>
</ul>
<h3>Title: SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes. (arXiv:2307.07333v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07333">http://arxiv.org/abs/2307.07333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07333] SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes](http://arxiv.org/abs/2307.07333) #segmentation</code></li>
<li>Summary: <p>In this work, we present SynTable, a unified and flexible Python-based
dataset generator built using NVIDIA's Isaac Sim Replicator Composer for
generating high-quality synthetic datasets for unseen object amodal instance
segmentation of cluttered tabletop scenes. Our dataset generation tool can
render a complex 3D scene containing object meshes, materials, textures,
lighting, and backgrounds. Metadata, such as modal and amodal instance
segmentation masks, occlusion masks, depth maps, bounding boxes, and material
properties, can be generated to automatically annotate the scene according to
the users' requirements. Our tool eliminates the need for manual labeling in
the dataset generation process while ensuring the quality and accuracy of the
dataset. In this work, we discuss our design goals, framework architecture, and
the performance of our tool. We demonstrate the use of a sample dataset
generated using SynTable by ray tracing for training a state-of-the-art model,
UOAIS-Net. The results show significantly improved performance in Sim-to-Real
transfer when evaluated on the OSD-Amodal dataset. We offer this tool as an
open-source, easy-to-use, photorealistic dataset generator for advancing
research in deep learning and synthetic data generation.
</p></li>
</ul>
<h3>Title: A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07362">http://arxiv.org/abs/2307.07362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07362] A scoping review on multimodal deep learning in biomedical images and texts](http://arxiv.org/abs/2307.07362) #segmentation</code></li>
<li>Summary: <p>Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers'
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: Deepfake Video Detection Using Generative Convolutional Vision Transformer. (arXiv:2307.07036v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07036">http://arxiv.org/abs/2307.07036</a></li>
<li>Code URL: <a href="https://github.com/erprogs/genconvit">https://github.com/erprogs/genconvit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07036] Deepfake Video Detection Using Generative Convolutional Vision Transformer](http://arxiv.org/abs/2307.07036) #extraction</code></li>
<li>Summary: <p>Deepfakes have raised significant concerns due to their potential to spread
false information and compromise digital media integrity. In this work, we
propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake
video detection. Our model combines ConvNeXt and Swin Transformer models for
feature extraction, and it utilizes Autoencoder and Variational Autoencoder to
learn from the latent data distribution. By learning from the visual artifacts
and latent data distribution, GenConViT achieves improved performance in
detecting a wide range of deepfake videos. The model is trained and evaluated
on DFDC, FF++, DeepfakeTIMIT, and Celeb-DF v2 datasets, achieving high
classification accuracy, F1 scores, and AUC values. The proposed GenConViT
model demonstrates robust performance in deepfake video detection, with an
average accuracy of 95.8% and an AUC value of 99.3% across the tested datasets.
Our proposed model addresses the challenge of generalizability in deepfake
detection by leveraging visual and latent features and providing an effective
solution for identifying a wide range of fake videos while preserving media
integrity. The code for GenConViT is available at
https://github.com/erprogs/GenConViT.
</p></li>
</ul>
<h3>Title: Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition. (arXiv:2307.07214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07214">http://arxiv.org/abs/2307.07214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07214] Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition](http://arxiv.org/abs/2307.07214) #extraction</code></li>
<li>Summary: <p>Open-set image recognition is a challenging topic in computer vision. Most of
the existing works in literature focus on learning more discriminative features
from the input images, however, they are usually insensitive to the high- or
low-frequency components in features, resulting in a decreasing performance on
fine-grained image recognition. To address this problem, we propose a
Complementary Frequency-varying Awareness Network that could better capture
both high-frequency and low-frequency information, called CFAN. The proposed
CFAN consists of three sequential modules: (i) a feature extraction module is
introduced for learning preliminary features from the input images; (ii) a
frequency-varying filtering module is designed to separate out both high- and
low-frequency components from the preliminary features in the frequency domain
via a frequency-adjustable filter; (iii) a complementary temporal aggregation
module is designed for aggregating the high- and low-frequency components via
two Long Short-Term Memory networks into discriminative features. Based on
CFAN, we further propose an open-set fine-grained image recognition method,
called CFAN-OSFGR, which learns image features via CFAN and classifies them via
a linear classifier. Experimental results on 3 fine-grained datasets and 2
coarse-grained datasets demonstrate that CFAN-OSFGR performs significantly
better than 9 state-of-the-art methods in most cases.
</p></li>
</ul>
<h3>Title: MaxSR: Image Super-Resolution Using Improved MaxViT. (arXiv:2307.07240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07240">http://arxiv.org/abs/2307.07240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07240] MaxSR: Image Super-Resolution Using Improved MaxViT](http://arxiv.org/abs/2307.07240) #extraction</code></li>
<li>Summary: <p>While transformer models have been demonstrated to be effective for natural
language processing tasks and high-level vision tasks, only a few attempts have
been made to use powerful transformer models for single image super-resolution.
Because transformer models have powerful representation capacity and the
in-built self-attention mechanisms in transformer models help to leverage
self-similarity prior in input low-resolution image to improve performance for
single image super-resolution, we present a single image super-resolution model
based on recent hybrid vision transformer of MaxViT, named as MaxSR. MaxSR
consists of four parts, a shallow feature extraction block, multiple cascaded
adaptive MaxViT blocks to extract deep hierarchical features and model global
self-similarity from low-level features efficiently, a hierarchical feature
fusion block, and finally a reconstruction block. The key component of MaxSR,
i.e., adaptive MaxViT block, is based on MaxViT block which mixes MBConv with
squeeze-and-excitation, block attention and grid attention. In order to achieve
better global modelling of self-similarity in input low-resolution image, we
improve block attention and grid attention in MaxViT block to adaptive block
attention and adaptive grid attention which do self-attention inside each
window across all grids and each grid across all windows respectively in the
most efficient way. We instantiate proposed model for classical single image
super-resolution (MaxSR) and lightweight single image super-resolution
(MaxSR-light). Experiments show that our MaxSR and MaxSR-light establish new
state-of-the-art performance efficiently.
</p></li>
</ul>
<h2>lidar</h2>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-17]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
