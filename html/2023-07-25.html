<h2>pointcloud</h2>
<h2>railway</h2>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds. (arXiv:2307.11914v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11914">http://arxiv.org/abs/2307.11914</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11914] Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds](http://arxiv.org/abs/2307.11914) #segmentation</code></li>
<li>Summary: <p>Urban modeling from LiDAR point clouds is an important topic in computer
vision, computer graphics, photogrammetry and remote sensing. 3D city models
have found a wide range of applications in smart cities, autonomous navigation,
urban planning and mapping etc. However, existing datasets for 3D modeling
mainly focus on common objects such as furniture or cars. Lack of building
datasets has become a major obstacle for applying deep learning technology to
specific domains such as urban modeling. In this paper, we present a
urban-scale dataset consisting of more than 160 thousands buildings along with
corresponding point clouds, mesh and wire-frame models, covering 16 cities in
Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art
algorithms including handcrafted and deep feature based methods. Experimental
results indicate that Building3D has challenges of high intra-class variance,
data imbalance and large-scale noises. The Building3D is the first and largest
urban-scale building modeling benchmark, allowing a comparison of supervised
and self-supervised learning methods. We believe that our Building3D will
facilitate future research on urban modeling, aerial path planning, mesh
simplification, and semantic/part segmentation etc.
</p></li>
</ul>
<h3>Title: Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation. (arXiv:2307.11958v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11958">http://arxiv.org/abs/2307.11958</a></li>
<li>Code URL: <a href="https://github.com/endoluminalsurgicalvision-imr/ccfv">https://github.com/endoluminalsurgicalvision-imr/ccfv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11958] Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation](http://arxiv.org/abs/2307.11958) #segmentation</code></li>
<li>Summary: <p>Transfer learning is a critical technique in training deep neural networks
for the challenging medical image segmentation task that requires enormous
resources. With the abundance of medical image data, many research institutions
release models trained on various datasets that can form a huge pool of
candidate source models to choose from. Hence, it's vital to estimate the
source models' transferability (i.e., the ability to generalize across
different downstream tasks) for proper and efficient model reuse. To make up
for its deficiency when applying transfer learning to medical image
segmentation, in this paper, we therefore propose a new Transferability
Estimation (TE) method. We first analyze the drawbacks of using the existing TE
algorithms for medical image segmentation and then design a source-free TE
framework that considers both class consistency and feature variety for better
estimation. Extensive experiments show that our method surpasses all current
algorithms for transferability estimation in medical image segmentation. Code
is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV
</p></li>
</ul>
<h3>Title: Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping. (arXiv:2307.11989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11989">http://arxiv.org/abs/2307.11989</a></li>
<li>Code URL: <a href="https://github.com/xmed-lab/mssg">https://github.com/xmed-lab/mssg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11989] Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping](http://arxiv.org/abs/2307.11989) #segmentation</code></li>
<li>Summary: <p>Designing deep learning algorithms for gland segmentation is crucial for
automatic cancer diagnosis and prognosis, yet the expensive annotation cost
hinders the development and application of this technology. In this paper, we
make a first attempt to explore a deep learning method for unsupervised gland
segmentation, where no manual annotations are required. Existing unsupervised
semantic segmentation methods encounter a huge challenge on gland images: They
either over-segment a gland into many fractions or under-segment the gland
regions by confusing many of them with the background. To overcome this
challenge, our key insight is to introduce an empirical cue about gland
morphology as extra knowledge to guide the segmentation process. To this end,
we propose a novel Morphology-inspired method via Selective Semantic Grouping.
We first leverage the empirical cue to selectively mine out proposals for gland
sub-regions with variant appearances. Then, a Morphology-aware Semantic
Grouping module is employed to summarize the overall information about the
gland by explicitly grouping the semantics of its sub-region proposals. In this
way, the final segmentation network could learn comprehensive knowledge about
glands and produce well-delineated, complete predictions. We conduct
experiments on GlaS dataset and CRAG dataset. Our method exceeds the
second-best counterpart over 10.56% at mIOU.
</p></li>
</ul>
<h3>Title: COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation. (arXiv:2307.12004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12004">http://arxiv.org/abs/2307.12004</a></li>
<li>Code URL: <a href="https://github.com/medicl-vu/colossal">https://github.com/medicl-vu/colossal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12004] COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation](http://arxiv.org/abs/2307.12004) #segmentation</code></li>
<li>Summary: <p>Medical image segmentation is a critical task in medical image analysis. In
recent years, deep learning based approaches have shown exceptional performance
when trained on a fully-annotated dataset. However, data annotation is often a
significant bottleneck, especially for 3D medical images. Active learning (AL)
is a promising solution for efficient annotation but requires an initial set of
labeled samples to start active selection. When the entire data pool is
unlabeled, how do we select the samples to annotate as our initial set? This is
also known as the cold-start AL, which permits only one chance to request
annotations from experts without access to previously annotated data.
Cold-start AL is highly relevant in many practical scenarios but has been
under-explored, especially for 3D medical segmentation tasks requiring
substantial annotation effort. In this paper, we present a benchmark named
COLosSAL by evaluating six cold-start AL strategies on five 3D medical image
segmentation tasks from the public Medical Segmentation Decathlon collection.
We perform a thorough performance analysis and explore important open questions
for cold-start AL, such as the impact of budget on different strategies. Our
results show that cold-start AL is still an unsolved problem for 3D
segmentation tasks but some important trends have been observed. The code
repository, data partitions, and baseline results for the complete benchmark
are publicly available at https://github.com/MedICL-VU/COLosSAL.
</p></li>
</ul>
<h3>Title: Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>Code URL: <a href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12032] Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space](http://arxiv.org/abs/2307.12032) #segmentation</code></li>
<li>Summary: <p>Air transport poses significant environmental challenges, particularly the
contribution of flight contrails to climate change due to their potential
global warming impact. Detecting contrails from satellite images has been a
long-standing challenge. Traditional computer vision techniques have
limitations under varying image conditions, and machine learning approaches
using typical convolutional neural networks are hindered by the scarcity of
hand-labeled contrail datasets and contrail-tailored learning processes. In
this paper, we introduce an innovative model based on augmented transfer
learning that accurately detects contrails with minimal data. We also propose a
novel loss function, SR Loss, which improves contrail line detection by
transforming the image space into Hough space. Our research opens new avenues
for machine learning-based contrail detection in aviation research, offering
solutions to the lack of large hand-labeled datasets, and significantly
enhancing contrail detection models.
</p></li>
</ul>
<h3>Title: Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data. (arXiv:2307.12033v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12033">http://arxiv.org/abs/2307.12033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12033] Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data](http://arxiv.org/abs/2307.12033) #segmentation</code></li>
<li>Summary: <p>Early detection of colorectal polyps is of utmost importance for their
treatment and for colorectal cancer prevention. Computer vision techniques have
the potential to aid professionals in the diagnosis stage, where colonoscopies
are manually carried out to examine the entirety of the patient's colon. The
main challenge in medical imaging is the lack of data, and a further challenge
specific to polyp segmentation approaches is the difficulty of manually
labeling the available data: the annotation process for segmentation tasks is
very time-consuming. While most recent approaches address the data availability
challenge with sophisticated techniques to better exploit the available labeled
data, few of them explore the self-supervised or semi-supervised paradigm,
where the amount of labeling required is greatly reduced. To address both
challenges, we leverage synthetic data and propose an end-to-end model for
polyp segmentation that integrates real and synthetic data to artificially
increase the size of the datasets and aid the training when unlabeled samples
are available. Concretely, our model, Pl-CUT-Seg, transforms synthetic images
with an image-to-image translation module and combines the resulting images
with real images to train a segmentation model, where we use model predictions
as pseudo-labels to better leverage unlabeled samples. Additionally, we propose
PL-CUT-Seg+, an improved version of the model that incorporates targeted
regularization to address the domain gap between real and synthetic images. The
models are evaluated on standard benchmarks for polyp segmentation and reach
state-of-the-art results in the self- and semi-supervised setups.
</p></li>
</ul>
<h3>Title: Hallucination Improves the Performance of Unsupervised Visual Representation Learning. (arXiv:2307.12168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12168">http://arxiv.org/abs/2307.12168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12168] Hallucination Improves the Performance of Unsupervised Visual Representation Learning](http://arxiv.org/abs/2307.12168) #segmentation</code></li>
<li>Summary: <p>Contrastive learning models based on Siamese structure have demonstrated
remarkable performance in self-supervised learning. Such a success of
contrastive learning relies on two conditions, a sufficient number of positive
pairs and adequate variations between them. If the conditions are not met,
these frameworks will lack semantic contrast and be fragile on overfitting. To
address these two issues, we propose Hallucinator that could efficiently
generate additional positive samples for further contrast. The Hallucinator is
differentiable and creates new data in the feature space. Thus, it is optimized
directly with the pre-training task and introduces nearly negligible
computation. Moreover, we reduce the mutual information of hallucinated pairs
and smooth them through non-linear operations. This process helps avoid
over-confident contrastive learning models during the training and achieves
more transformation-invariant feature embeddings. Remarkably, we empirically
prove that the proposed Hallucinator generalizes well to various contrastive
learning models, including MoCoV1&amp;V2, SimCLR and SimSiam. Under the linear
classification protocol, a stable accuracy gain is achieved, ranging from 0.3%
to 3.0% on CIFAR10&amp;100, Tiny ImageNet, STL-10 and ImageNet. The improvement is
also observed in transferring pre-train encoders to the downstream tasks,
including object detection and segmentation.
</p></li>
</ul>
<h3>Title: DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space. (arXiv:2307.12208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12208">http://arxiv.org/abs/2307.12208</a></li>
<li>Code URL: <a href="https://github.com/haonanguo/deepcl">https://github.com/haonanguo/deepcl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12208] DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space](http://arxiv.org/abs/2307.12208) #segmentation</code></li>
<li>Summary: <p>Change detection (CD) is an important yet challenging task in the Earth
observation field for monitoring Earth surface dynamics. The advent of deep
learning techniques has recently propelled automatic CD into a technological
revolution. Nevertheless, deep learning-based CD methods are still plagued by
two primary issues: 1) insufficient temporal relationship modeling and 2)
pseudo-change misclassification. To address these issues, we complement the
strong temporal modeling ability of metric learning with the prominent fitting
ability of segmentation and propose a deep change feature learning (DeepCL)
framework for robust and explainable CD. Firstly, we designed a hard
sample-aware contrastive loss, which reweights the importance of hard and
simple samples. This loss allows for explicit modeling of the temporal
correlation between bi-temporal remote sensing images. Furthermore, the modeled
temporal relations are utilized as knowledge prior to guide the segmentation
process for detecting change regions. The DeepCL framework is thoroughly
evaluated both theoretically and experimentally, demonstrating its superior
feature discriminability, resilience against pseudo changes, and adaptability
to a variety of CD algorithms. Extensive comparative experiments substantiate
the quantitative and qualitative superiority of DeepCL over state-of-the-art CD
approaches.
</p></li>
</ul>
<h3>Title: Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision. (arXiv:2307.12220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12220">http://arxiv.org/abs/2307.12220</a></li>
<li>Code URL: <a href="https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework">https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12220] Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision](http://arxiv.org/abs/2307.12220) #segmentation</code></li>
<li>Summary: <p>The efficacy of building footprint segmentation from remotely sensed images
has been hindered by model transfer effectiveness. Many existing building
segmentation methods were developed upon the encoder-decoder architecture of
U-Net, in which the encoder is finetuned from the newly developed backbone
networks that are pre-trained on ImageNet. However, the heavy computational
burden of the existing decoder designs hampers the successful transfer of these
modern encoder networks to remote sensing tasks. Even the widely-adopted deep
supervision strategy fails to mitigate these challenges due to its invalid loss
in hybrid regions where foreground and background pixels are intermixed. In
this paper, we conduct a comprehensive evaluation of existing decoder network
designs for building footprint segmentation and propose an efficient framework
denoted as BFSeg to enhance learning efficiency and effectiveness.
Specifically, a densely-connected coarse-to-fine feature fusion decoder network
that facilitates easy and fast feature fusion across scales is proposed.
Moreover, considering the invalidity of hybrid regions in the down-sampled
ground truth during the deep supervision process, we present a lenient deep
supervision and distillation strategy that enables the network to learn proper
knowledge from deep supervision. Building upon these advancements, we have
developed a new family of building segmentation networks, which consistently
surpass prior works with outstanding performance and efficiency across a wide
range of newly developed encoder networks. The code will be released on
https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h3>Title: Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach. (arXiv:2307.12049v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12049">http://arxiv.org/abs/2307.12049</a></li>
<li>Code URL: <a href="https://github.com/wenc13/patchgeneration">https://github.com/wenc13/patchgeneration</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12049] Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach](http://arxiv.org/abs/2307.12049) #3d point cloud</code></li>
<li>Summary: <p>A generative model for high-fidelity point clouds is of great importance in
synthesizing 3d environments for applications such as autonomous driving and
robotics. Despite the recent success of deep generative models for 2d images,
it is non-trivial to generate 3d point clouds without a comprehensive
understanding of both local and global geometric structures. In this paper, we
devise a new 3d point cloud generation framework using a divide-and-conquer
approach, where the whole generation process can be divided into a set of
patch-wise generation tasks. Specifically, all patch generators are based on
learnable priors, which aim to capture the information of geometry primitives.
We introduce point- and patch-wise transformers to enable the interactions
between points and patches. Therefore, the proposed divide-and-conquer approach
contributes to a new understanding of point cloud generation from the geometry
constitution of 3d shapes. Experimental results on a variety of object
categories from the most popular point cloud dataset, ShapeNet, show the
effectiveness of the proposed patch-wise point cloud generation, where it
clearly outperforms recent state-of-the-art methods for high-fidelity point
cloud generation.
</p></li>
</ul>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h2>lidar</h2>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-25]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
