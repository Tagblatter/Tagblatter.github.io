<h2>pointcloud</h2>
<h2>railway</h2>
<h3>Title: Line-Constrained $k$-Semi-Obnoxious Facility Location. (arXiv:2307.03488v1 [cs.CG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03488">http://arxiv.org/abs/2307.03488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03488] Line-Constrained $k$-Semi-Obnoxious Facility Location](http://arxiv.org/abs/2307.03488) #railway</code></li>
<li>Summary: <p>Suppose we are given a set $\cal B$ of blue points and a set $\cal R$ of red
points, all lying above a horizontal line $\ell$, in the plane. Let the weight
of a given point $p_i\in {\cal B}\cup{\cal R}$ be $w_i>0$ if $p_i\in {\cal B}$
and $w_i<0$ if $p_i\in {\cal R}$, $|{\cal B}\cup{\cal R}|=n$, and
$d^0$($=d\setminus\partial d$) be the interior of any geometric object $d$. We
wish to pack $k$ non-overlapping congruent disks $d_1$, $d_2$, \ldots, $d_k$ of
minimum radius, centered on $\ell$ such that
$\sum\limits_{j=1}^k\sum\limits_{{i:\exists p_i\in{\cal R}, p_i\in
d_j^0}}w_i+\sum\limits_{j=1}^k\sum\limits_{{i:\exists p_i\in{\cal B}, p_i\in
d_j}}w_i$ is maximized, i.e., the sum of the weights of the points covered by
$\bigcup\limits_{j=1}^kd_j$ is maximized. Here, the disks are the obnoxious or
undesirable facilities generating nuisance or damage (with quantity equal to
$w_i$) to every demand point (e.g., population center) $p_i\in {\cal R}$ lying
in their interior. In contrast, they are the desirable facilities giving
service (equal to $w_i$) to every demand point $p_i\in {\cal B}$ covered by
them. The line $\ell$ represents a straight highway or railway line. These $k$
semi-obnoxious facilities need to be established on $\ell$ to receive the
largest possible overall service for the nearby attractive demand points while
causing minimum damage to the nearby repelling demand points. We show that the
problem can be solved optimally in $O(n^4k^2)$ time. Subsequently, we improve
the running time to $O(n^3k \cdot\max{(\log n, k)})$. The above-weighted
variation of locating $k$ semi-obnoxious facilities may generalize the problem
that Bereg et al. (2015) studied where $k=1$ i.e., the smallest radius maximum
weight circle is to be centered on a line. Furthermore, we addressed two
special cases of the problem where points do not have arbitrary weights.
</p></li>
</ul>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images. (arXiv:2307.03273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03273">http://arxiv.org/abs/2307.03273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03273] ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images](http://arxiv.org/abs/2307.03273) #segmentation</code></li>
<li>Summary: <p>Statistical shape models (SSM) have been well-established as an excellent
tool for identifying variations in the morphology of anatomy across the
underlying population. Shape models use consistent shape representation across
all the samples in a given cohort, which helps to compare shapes and identify
the variations that can detect pathologies and help in formulating treatment
plans. In medical imaging, computing these shape representations from CT/MRI
scans requires time-intensive preprocessing operations, including but not
limited to anatomy segmentation annotations, registration, and texture
denoising. Deep learning models have demonstrated exceptional capabilities in
learning shape representations directly from volumetric images, giving rise to
highly effective and efficient Image-to-SSM. Nevertheless, these models are
data-hungry and due to the limited availability of medical data, deep learning
models tend to overfit. Offline data augmentation techniques, that use kernel
density estimation based (KDE) methods for generating shape-augmented samples,
have successfully aided Image-to-SSM networks in achieving comparable accuracy
to traditional SSM methods. However, these augmentation methods focus on shape
augmentation, whereas deep learning models exhibit texture bias results in
sub-optimal models. This paper introduces a novel strategy for on-the-fly data
augmentation for the Image-to-SSM framework by leveraging data-dependent noise
generation or texture augmentation. The proposed framework is trained as an
adversary to the Image-to-SSM network, augmenting diverse and challenging noisy
samples. Our approach achieves improved accuracy by encouraging the model to
focus on the underlying geometry rather than relying solely on pixel values.
</p></li>
</ul>
<h3>Title: To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology. (arXiv:2307.03275v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03275">http://arxiv.org/abs/2307.03275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03275] To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology](http://arxiv.org/abs/2307.03275) #segmentation</code></li>
<li>Summary: <p>Annotating medical imaging datasets is costly, so fine-tuning (or transfer
learning) is the most effective method for digital pathology vision
applications such as disease classification and semantic segmentation. However,
due to texture bias in models trained on real-world images, transfer learning
for histopathology applications might result in underperforming models, which
necessitates the need for using unlabeled histopathology data and
self-supervised methods to discover domain-specific characteristics. Here, we
tested the premise that histopathology-specific pretrained models provide
better initializations for pathology vision tasks, i.e., gland and cell
segmentation. In this study, we compare the performance of gland and cell
segmentation tasks with domain-specific and non-domain-specific pretrained
weights. Moreover, we investigate the data size at which domain-specific
pretraining produces a statistically significant difference in performance. In
addition, we investigated whether domain-specific initialization improves the
effectiveness of out-of-domain testing on distinct datasets but the same task.
The results indicate that performance gain using domain-specific pretraining
depends on both the task and the size of the training dataset. In instances
with limited dataset sizes, a significant improvement in gland segmentation
performance was also observed, whereas models trained on cell segmentation
datasets exhibit no improvement.
</p></li>
</ul>
<h3>Title: Weakly-supervised Contrastive Learning for Unsupervised Object Discovery. (arXiv:2307.03376v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03376">http://arxiv.org/abs/2307.03376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03376] Weakly-supervised Contrastive Learning for Unsupervised Object Discovery](http://arxiv.org/abs/2307.03376) #segmentation</code></li>
<li>Summary: <p>Unsupervised object discovery (UOD) refers to the task of discriminating the
whole region of objects from the background within a scene without relying on
labeled datasets, which benefits the task of bounding-box-level localization
and pixel-level segmentation. This task is promising due to its ability to
discover objects in a generic manner. We roughly categorise existing techniques
into two main directions, namely the generative solutions based on image
resynthesis, and the clustering methods based on self-supervised models. We
have observed that the former heavily relies on the quality of image
reconstruction, while the latter shows limitations in effectively modeling
semantic correlations. To directly target at object discovery, we focus on the
latter approach and propose a novel solution by incorporating weakly-supervised
contrastive learning (WCL) to enhance semantic information exploration. We
design a semantic-guided self-supervised learning model to extract high-level
semantic features from images, which is achieved by fine-tuning the feature
encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we
introduce Principal Component Analysis (PCA) to localize object regions. The
principal projection direction, corresponding to the maximal eigenvalue, serves
as an indicator of the object region(s). Extensive experiments on benchmark
unsupervised object discovery datasets demonstrate the effectiveness of our
proposed solution. The source code and experimental results are publicly
available via our project page at https://github.com/npucvr/WSCUOD.git.
</p></li>
</ul>
<h3>Title: General-Purpose Multimodal Transformer meets Remote Sensing Semantic Segmentation. (arXiv:2307.03388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03388">http://arxiv.org/abs/2307.03388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03388] General-Purpose Multimodal Transformer meets Remote Sensing Semantic Segmentation](http://arxiv.org/abs/2307.03388) #segmentation</code></li>
<li>Summary: <p>The advent of high-resolution multispectral/hyperspectral sensors, LiDAR DSM
(Digital Surface Model) information and many others has provided us with an
unprecedented wealth of data for Earth Observation. Multimodal AI seeks to
exploit those complementary data sources, particularly for complex tasks like
semantic segmentation. While specialized architectures have been developed,
they are highly complicated via significant effort in model design, and require
considerable re-engineering whenever a new modality emerges. Recent trends in
general-purpose multimodal networks have shown great potential to achieve
state-of-the-art performance across multiple multimodal tasks with one unified
architecture. In this work, we investigate the performance of PerceiverIO, one
in the general-purpose multimodal family, in the remote sensing semantic
segmentation domain. Our experiments reveal that this ostensibly universal
network struggles with object scale variation in remote sensing images and
fails to detect the presence of cars from a top-down view. To address these
issues, even with extreme class imbalance issues, we propose a spatial and
volumetric learning component. Specifically, we design a UNet-inspired module
that employs 3D convolution to encode vital local information and learn
cross-modal features simultaneously, while reducing network computational
burden via the cross-attention mechanism of PerceiverIO. The effectiveness of
the proposed component is validated through extensive experiments comparing it
with other methods such as 2D convolution, and dual local module (\ie the
combination of Conv2D 1x1 and Conv2D 3x3 inspired by UNetFormer). The proposed
method achieves competitive results with specialized architectures like
UNetFormer and SwinUNet, showing its potential to minimize network architecture
engineering with a minimal compromise on the performance.
</p></li>
</ul>
<h3>Title: Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation. (arXiv:2307.03407v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03407">http://arxiv.org/abs/2307.03407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03407] Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation](http://arxiv.org/abs/2307.03407) #segmentation</code></li>
<li>Summary: <p>We address the task of weakly-supervised few-shot image classification and
segmentation, by leveraging a Vision Transformer (ViT) pretrained with
self-supervision. Our proposed method takes token representations from the
self-supervised ViT and leverages their correlations, via self-attention, to
produce classification and segmentation predictions through separate task
heads. Our model is able to effectively learn to perform classification and
segmentation in the absence of pixel-level labels during training, using only
image-level labels. To do this it uses attention maps, created from tokens
generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We
also explore a practical setup with ``mixed" supervision, where a small number
of training images contains ground-truth pixel-level labels and the remaining
images have only image-level labels. For this mixed setup, we propose to
improve the pseudo-labels using a pseudo-label enhancer that was trained using
the available ground-truth pixel-level labels. Experiments on Pascal-5i and
COCO-20i demonstrate significant performance gains in a variety of supervision
settings, and in particular when little-to-no pixel-level labels are available.
</p></li>
</ul>
<h3>Title: A Deep Active Contour Model for Delineating Glacier Calving Fronts. (arXiv:2307.03461v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03461">http://arxiv.org/abs/2307.03461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03461] A Deep Active Contour Model for Delineating Glacier Calving Fronts](http://arxiv.org/abs/2307.03461) #segmentation</code></li>
<li>Summary: <p>Choosing how to encode a real-world problem as a machine learning task is an
important design decision in machine learning. The task of glacier calving
front modeling has often been approached as a semantic segmentation task.
Recent studies have shown that combining segmentation with edge detection can
improve the accuracy of calving front detectors. Building on this observation,
we completely rephrase the task as a contour tracing problem and propose a
model for explicit contour detection that does not incorporate any dense
predictions as intermediate steps. The proposed approach, called ``Charting
Outlines by Recurrent Adaptation'' (COBRA), combines Convolutional Neural
Networks (CNNs) for feature extraction and active contour models for the
delineation. By training and evaluating on several large-scale datasets of
Greenland's outlet glaciers, we show that this approach indeed outperforms the
aforementioned methods based on segmentation and edge-detection. Finally, we
demonstrate that explicit contour detection has benefits over pixel-wise
methods when quantifying the models' prediction uncertainties. The project page
containing the code and animated model predictions can be found at
\url{https://khdlr.github.io/COBRA/}.
</p></li>
</ul>
<h3>Title: TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03465">http://arxiv.org/abs/2307.03465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03465] TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning](http://arxiv.org/abs/2307.03465) #segmentation</code></li>
<li>Summary: <p>The AllInOne training paradigm squeezes a wide range of tasks into a unified
model in a multi-task learning manner. However, optimization in multi-task
learning is more challenge than single-task learning, as the gradient norm from
different tasks may vary greatly, making the backbone overly biased towards one
specific task. To address this issue, we propose the task-level
backbone-oriented gradient clip paradigm, compared with the vanilla gradient
clip method, it has two points of emphasis:1) gradient clip is performed
independently for each task. 2) backbone gradients generated from each task are
rescaled to the same norm scale. Based on the experimental results, we argue
that the task-level backbone-oriented gradient clip paradigm can relieve the
gradient bias problem to some extent. We also propose a novel multi-branch data
augmentation strategy where conflict augmentations are placed in different
branches. Our approach has been shown to be effective and finally achieve 1st
place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023
Foundation Model Challenge. It's worth noting that instead of evaluating all
three tasks(detection, segmentation and fine-grained classification) in
Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in
which our team has a huge advantage.
</p></li>
</ul>
<h3>Title: Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03512">http://arxiv.org/abs/2307.03512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03512] Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data](http://arxiv.org/abs/2307.03512) #segmentation</code></li>
<li>Summary: <p>When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
</p></li>
</ul>
<h3>Title: Unsupervised Segmentation of Fetal Brain MRI using Deep Learning Cascaded Registration. (arXiv:2307.03579v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03579">http://arxiv.org/abs/2307.03579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03579] Unsupervised Segmentation of Fetal Brain MRI using Deep Learning Cascaded Registration](http://arxiv.org/abs/2307.03579) #segmentation</code></li>
<li>Summary: <p>Accurate segmentation of fetal brain magnetic resonance images is crucial for
analyzing fetal brain development and detecting potential neurodevelopmental
abnormalities. Traditional deep learning-based automatic segmentation, although
effective, requires extensive training data with ground-truth labels, typically
produced by clinicians through a time-consuming annotation process. To overcome
this challenge, we propose a novel unsupervised segmentation method based on
multi-atlas segmentation, that accurately segments multiple tissues without
relying on labeled data for training. Our method employs a cascaded deep
learning network for 3D image registration, which computes small, incremental
deformations to the moving image to align it precisely with the fixed image.
This cascaded network can then be used to register multiple annotated images
with the image to be segmented, and combine the propagated labels to form a
refined segmentation. Our experiments demonstrate that the proposed cascaded
architecture outperforms the state-of-the-art registration methods that were
tested. Furthermore, the derived segmentation method achieves similar
performance and inference time to nnU-Net while only using a small subset of
annotated data for the multi-atlas segmentation task and none for training the
network. Our pipeline for registration and multi-atlas segmentation is publicly
available at https://github.com/ValBcn/CasReg.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: Adaptive Generation of Privileged Intermediate Information for Visible-Infrared Person Re-Identification. (arXiv:2307.03240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03240">http://arxiv.org/abs/2307.03240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03240] Adaptive Generation of Privileged Intermediate Information for Visible-Infrared Person Re-Identification](http://arxiv.org/abs/2307.03240) #extraction</code></li>
<li>Summary: <p>Visible-infrared person re-identification seeks to retrieve images of the
same individual captured over a distributed network of RGB and IR sensors.
Several V-I ReID approaches directly integrate both V and I modalities to
discriminate persons within a shared representation space. However, given the
significant gap in data distributions between V and I modalities, cross-modal
V-I ReID remains challenging. Some recent approaches improve generalization by
leveraging intermediate spaces that can bridge V and I modalities, yet
effective methods are required to select or generate data for such informative
domains. In this paper, the Adaptive Generation of Privileged Intermediate
Information training approach is introduced to adapt and generate a virtual
domain that bridges discriminant information between the V and I modalities.
The key motivation behind AGPI^2 is to enhance the training of a deep V-I ReID
backbone by generating privileged images that provide additional information.
These privileged images capture shared discriminative features that are not
easily accessible within the original V or I modalities alone. Towards this
goal, a non-linear generative module is trained with an adversarial objective,
translating V images into intermediate spaces with a smaller domain shift
w.r.t. the I domain. Meanwhile, the embedding module within AGPI^2 aims to
produce similar features for both V and generated images, encouraging the
extraction of features that are common to all modalities. In addition to these
contributions, AGPI^2 employs adversarial objectives for adapting the
intermediate images, which play a crucial role in creating a
non-modality-specific space to address the large domain shifts between V and I
domains. Experimental results conducted on challenging V-I ReID datasets
indicate that AGPI^2 increases matching accuracy without extra computational
resources during inference.
</p></li>
</ul>
<h3>Title: Open-Vocabulary Object Detection via Scene Graph Discovery. (arXiv:2307.03339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03339">http://arxiv.org/abs/2307.03339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03339] Open-Vocabulary Object Detection via Scene Graph Discovery](http://arxiv.org/abs/2307.03339) #extraction</code></li>
<li>Summary: <p>In recent years, open-vocabulary (OV) object detection has attracted
increasing research attention. Unlike traditional detection, which only
recognizes fixed-category objects, OV detection aims to detect objects in an
open category set. Previous works often leverage vision-language (VL) training
data (e.g., referring grounding data) to recognize OV objects. However, they
only use pairs of nouns and individual objects in VL data, while these data
usually contain much more information, such as scene graphs, which are also
crucial for OV detection. In this paper, we propose a novel Scene-Graph-Based
Discovery Network (SGDN) that exploits scene graph cues for OV detection.
Firstly, a scene-graph-based decoder (SGDecoder) including sparse
scene-graph-guided attention (SSGA) is presented. It captures scene graphs and
leverages them to discover OV objects. Secondly, we propose scene-graph-based
prediction (SGPred), where we build a scene-graph-based offset regression
(SGOR) mechanism to enable mutual enhancement between scene graph extraction
and object localization. Thirdly, we design a cross-modal learning mechanism in
SGPred. It takes scene graphs as bridges to improve the consistency between
cross-modal embeddings for OV object classification. Experiments on COCO and
LVIS demonstrate the effectiveness of our approach. Moreover, we show the
ability of our model for OV scene graph detection, while previous OV scene
graph generation methods cannot tackle this task.
</p></li>
</ul>
<h3>Title: All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03373">http://arxiv.org/abs/2307.03373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03373] All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment](http://arxiv.org/abs/2307.03373) #extraction</code></li>
<li>Summary: <p>Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.
</p></li>
</ul>
<h3>Title: Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery. (arXiv:2307.03398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03398">http://arxiv.org/abs/2307.03398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03398] Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery](http://arxiv.org/abs/2307.03398) #extraction</code></li>
<li>Summary: <p>Street-view imagery provides us with novel experiences to explore different
places remotely. Carefully calibrated street-view images (e.g. Google Street
View) can be used for different downstream tasks, e.g. navigation, map features
extraction. As personal high-quality cameras have become much more affordable
and portable, an enormous amount of crowdsourced street-view images are
uploaded to the internet, but commonly with missing or noisy sensor
information. To prepare this hidden treasure for "ready-to-use" status,
determining missing location information and camera orientation angles are two
equally important tasks. Recent methods have achieved high performance on
geo-localization of street-view images by cross-view matching with a pool of
geo-referenced satellite imagery. However, most of the existing works focus
more on geo-localization than estimating the image orientation. In this work,
we re-state the importance of finding fine-grained orientation for street-view
images, formally define the problem and provide a set of evaluation metrics to
assess the quality of the orientation estimation. We propose two methods to
improve the granularity of the orientation estimation, achieving 82.4% and
72.3% accuracy for images with estimated angle errors below 2 degrees for CVUSA
and CVACT datasets, corresponding to 34.9% and 28.2% absolute improvement
compared to previous works. Integrating fine-grained orientation estimation in
training also improves the performance on geo-localization, giving top 1 recall
95.5%/85.5% and 86.8%/80.4% for orientation known/unknown tests on the two
datasets.
</p></li>
</ul>
<h2>lidar</h2>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-10]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
