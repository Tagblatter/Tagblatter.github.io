<h2>pointcloud</h2>
<h2>railway</h2>
<h2>bim</h2>
<h2>procedural modeling</h2>
<h2>segmentation</h2>
<h3>Title: DenseMP: Unsupervised Dense Pre-training for Few-shot Medical Image Segmentation. (arXiv:2307.09604v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09604">http://arxiv.org/abs/2307.09604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09604] DenseMP: Unsupervised Dense Pre-training for Few-shot Medical Image Segmentation](http://arxiv.org/abs/2307.09604) #segmentation</code></li>
<li>Summary: <p>Few-shot medical image semantic segmentation is of paramount importance in
the domain of medical image analysis. However, existing methodologies grapple
with the challenge of data scarcity during the training phase, leading to
over-fitting. To mitigate this issue, we introduce a novel Unsupervised Dense
Few-shot Medical Image Segmentation Model Training Pipeline (DenseMP) that
capitalizes on unsupervised dense pre-training. DenseMP is composed of two
distinct stages: (1) segmentation-aware dense contrastive pre-training, and (2)
few-shot-aware superpixel guided dense pre-training. These stages
collaboratively yield a pre-trained initial model specifically designed for
few-shot medical image segmentation, which can subsequently be fine-tuned on
the target dataset. Our proposed pipeline significantly enhances the
performance of the widely recognized few-shot segmentation model, PA-Net,
achieving state-of-the-art results on the Abd-CT and Abd-MRI datasets. Code
will be released after acceptance.
</p></li>
</ul>
<h3>Title: ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations. (arXiv:2307.09732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09732">http://arxiv.org/abs/2307.09732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09732] ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations](http://arxiv.org/abs/2307.09732) #segmentation</code></li>
<li>Summary: <p>3D instance segmentation methods often require fully-annotated dense labels
for training, which are costly to obtain. In this paper, we present ClickSeg, a
novel click-level weakly supervised 3D instance segmentation method that
requires one point per instance annotation merely. Such a problem is very
challenging due to the extremely limited labels, which has rarely been solved
before. We first develop a baseline weakly-supervised training method, which
generates pseudo labels for unlabeled data by the model itself. To utilize the
property of click-level annotation setting, we further propose a new training
framework. Instead of directly using the model inference way, i.e., mean-shift
clustering, to generate the pseudo labels, we propose to use k-means with fixed
initial seeds: the annotated points. New similarity metrics are further
designed for clustering. Experiments on ScanNetV2 and S3DIS datasets show that
the proposed ClickSeg surpasses the previous best weakly supervised instance
segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2). Using
0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy
of the fully-supervised counterpart. Meanwhile, it also achieves
state-of-the-art semantic segmentation results among weakly supervised methods
that use the same annotation settings.
</p></li>
</ul>
<h3>Title: Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation. (arXiv:2307.09755v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09755">http://arxiv.org/abs/2307.09755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09755] Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation](http://arxiv.org/abs/2307.09755) #segmentation</code></li>
<li>Summary: <p>Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model
with limited labeled images and a substantial volume of unlabeled images. To
improve the robustness of representations, powerful methods introduce a
pixel-wise contrastive learning approach in latent space (i.e., representation
space) that aggregates the representations to their prototypes in a fully
supervised manner. However, previous contrastive-based S4 methods merely rely
on the supervision from the model's output (logits) in logit space during
unlabeled training. In contrast, we utilize the outputs in both logit space and
representation space to obtain supervision in a collaborative way. The
supervision from two spaces plays two roles: 1) reduces the risk of
over-fitting to incorrect semantic information in logits with the help of
representations; 2) enhances the knowledge exchange between the two spaces.
Furthermore, unlike previous approaches, we use the similarity between
representations and prototypes as a new indicator to tilt training those
under-performing representations and achieve a more efficient contrastive
learning process. Results on two public benchmarks demonstrate the competitive
performance of our method compared with state-of-the-art methods.
</p></li>
</ul>
<h3>Title: Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning. (arXiv:2307.09769v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09769">http://arxiv.org/abs/2307.09769</a></li>
<li>Code URL: <a href="https://github.com/cscyqj/miccai23-protocontra-sfda">https://github.com/cscyqj/miccai23-protocontra-sfda</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09769] Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning](http://arxiv.org/abs/2307.09769) #segmentation</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) has increasingly gained interests for
its capacity to transfer the knowledge learned from a labeled source domain to
an unlabeled target domain. However, typical UDA methods require concurrent
access to both the source and target domain data, which largely limits its
application in medical scenarios where source data is often unavailable due to
privacy concern. To tackle the source data-absent problem, we present a novel
two-stage source-free domain adaptation (SFDA) framework for medical image
segmentation, where only a well-trained source segmentation model and unlabeled
target data are available during domain adaptation. Specifically, in the
prototype-anchored feature alignment stage, we first utilize the weights of the
pre-trained pixel-wise classifier as source prototypes, which preserve the
information of source features. Then, we introduce the bi-directional transport
to align the target features with class prototypes by minimizing its expected
cost. On top of that, a contrastive learning stage is further devised to
utilize those pixels with unreliable predictions for a more compact target
feature distribution. Extensive experiments on a cross-modality medical
segmentation task demonstrate the superiority of our method in large domain
discrepancy settings compared with the state-of-the-art SFDA approaches and
even some UDA methods. Code is available at
https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.
</p></li>
</ul>
<h3>Title: Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09781">http://arxiv.org/abs/2307.09781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09781] Text2Layer: Layered Image Generation using Latent Diffusion Model](http://arxiv.org/abs/2307.09781) #segmentation</code></li>
<li>Summary: <p>Layer compositing is one of the most popular image editing workflows among
both amateurs and professionals. Motivated by the success of diffusion models,
we explore layer compositing from a layered image generation perspective.
Instead of generating an image, we propose to generate background, foreground,
layer mask, and the composed image simultaneously. To achieve layered image
generation, we train an autoencoder that is able to reconstruct layered images
and train diffusion models on the latent representation. One benefit of the
proposed problem is to enable better compositing workflows in addition to the
high-quality image output. Another benefit is producing higher-quality layer
masks compared to masks produced by a separate step of image segmentation.
Experimental results show that the proposed method is able to generate
high-quality layered images and initiates a benchmark for future work.
</p></li>
</ul>
<h3>Title: DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis. (arXiv:2307.09787v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09787">http://arxiv.org/abs/2307.09787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09787] DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis](http://arxiv.org/abs/2307.09787) #segmentation</code></li>
<li>Summary: <p>Limited labeled data makes it hard to train models from scratch in medical
domain, and an important paradigm is pre-training and then fine-tuning. Large
pre-trained models contain rich representations, which can be adapted to
downstream medical tasks. However, existing methods either tune all the
parameters or the task-specific layers of the pre-trained models, ignoring the
input variations of medical images, and thus they are not efficient or
effective. In this work, we aim to study parameter-efficient fine-tuning (PEFT)
for medical image analysis, and propose a dynamic visual prompt tuning method,
named DVPT. It can extract knowledge beneficial to downstream tasks from large
models with a few trainable parameters. Firstly, the frozen features are
transformed by an lightweight bottleneck layer to learn the domain-specific
distribution of downstream medical tasks, and then a few learnable visual
prompts are used as dynamic queries and then conduct cross-attention with the
transformed features, attempting to acquire sample-specific knowledge that are
suitable for each sample. Finally, the features are projected to original
feature dimension and aggregated with the frozen features. This DVPT module can
be shared between different Transformer layers, further reducing the trainable
parameters. To validate DVPT, we conduct extensive experiments with different
pre-trained models on medical classification and segmentation tasks. We find
such PEFT method can not only efficiently adapt the pre-trained models to the
medical domain, but also brings data efficiency with partial labeled data. For
example, with 0.5\% extra trainable parameters, our method not only outperforms
state-of-the-art PEFT methods, even surpasses the full fine-tuning by more than
2.20\% Kappa score on medical classification task. It can saves up to 60\%
labeled data and 99\% storage cost of ViT-B/16.
</p></li>
</ul>
<h3>Title: U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09947">http://arxiv.org/abs/2307.09947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09947] U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation](http://arxiv.org/abs/2307.09947) #segmentation</code></li>
<li>Summary: <p>Deep neural networks have shown exceptional performance in various tasks, but
their lack of robustness, reliability, and tendency to be overconfident pose
challenges for their deployment in safety-critical applications like autonomous
driving. In this regard, quantifying the uncertainty inherent to a model's
prediction is a promising endeavour to address these shortcomings. In this
work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that
incorporates dynamic predictive uncertainties into the training process by
pixel-wise weighting of the well-known cross-entropy loss (CE). Through
extensive experimentation, we demonstrate the superiority of U-CE over regular
CE training on two benchmark datasets, Cityscapes and ACDC, using two common
backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train
models that not only improve their segmentation performance but also provide
meaningful uncertainties after training. Consequently, we contribute to the
development of more robust and reliable segmentation models, ultimately
advancing the state-of-the-art in safety-critical applications and beyond.
</p></li>
</ul>
<h3>Title: Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher. (arXiv:2307.09973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09973">http://arxiv.org/abs/2307.09973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09973] Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher](http://arxiv.org/abs/2307.09973) #segmentation</code></li>
<li>Summary: <p>This paper studies source-free domain adaptive fundus image segmentation
which aims to adapt a pretrained fundus segmentation model to a target domain
using unlabeled images. This is a challenging task because it is highly risky
to adapt a model only using unlabeled data. Most existing methods tackle this
task mainly by designing techniques to carefully generate pseudo labels from
the model's predictions and use the pseudo labels to train the model. While
often obtaining positive adaption effects, these methods suffer from two major
issues. First, they tend to be fairly unstable - incorrect pseudo labels
abruptly emerged may cause a catastrophic impact on the model. Second, they
fail to consider the severe class imbalance of fundus images where the
foreground (e.g., cup) region is usually very small. This paper aims to address
these two issues by proposing the Class-Balanced Mean Teacher (CBMT) model.
CBMT addresses the unstable issue by proposing a weak-strong augmented mean
teacher learning scheme where only the teacher model generates pseudo labels
from weakly augmented images to train a student model that takes strongly
augmented images as input. The teacher is updated as the moving average of the
instantly trained student, which could be noisy. This prevents the teacher
model from being abruptly impacted by incorrect pseudo-labels. For the class
imbalance issue, CBMT proposes a novel loss calibration approach to highlight
foreground classes according to global statistics. Experiments show that CBMT
well addresses these two issues and outperforms existing methods on multiple
benchmarks.
</p></li>
</ul>
<h3>Title: Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition. (arXiv:2307.10036v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.10036">http://arxiv.org/abs/2307.10036</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.10036] Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition](http://arxiv.org/abs/2307.10036) #segmentation</code></li>
<li>Summary: <p>Automated medical image classification is the key component in intelligent
diagnosis systems. However, most medical image datasets contain plenty of
samples of common diseases and just a handful of rare ones, leading to major
class imbalances. Currently, it is an open problem in intelligent diagnosis to
effectively learn from imbalanced training data. In this paper, we propose a
simple yet effective framework, named \textbf{C}lass \textbf{A}ttention to
\textbf{RE}gions of the lesion (CARE), to handle data imbalance issues by
embedding attention into the training process of \textbf{C}onvolutional
\textbf{N}eural \textbf{N}etworks (CNNs). The proposed attention module helps
CNNs attend to lesion regions of rare diseases, therefore helping CNNs to learn
their characteristics more effectively. In addition, this attention module
works only during the training phase and does not change the architecture of
the original network, so it can be directly combined with any existing CNN
architecture. The CARE framework needs bounding boxes to represent the lesion
regions of rare diseases. To alleviate the need for manual annotation, we
further developed variants of CARE by leveraging the traditional saliency
methods or a pretrained segmentation model for bounding box generation. Results
show that the CARE variants with automated bounding box generation are
comparable to the original CARE framework with \textit{manual} bounding box
annotations. A series of experiments on an imbalanced skin image dataset and a
pneumonia dataset indicates that our method can effectively help the network
focus on the lesion regions of rare diseases and remarkably improves the
classification performance of rare diseases.
</p></li>
</ul>
<h3>Title: Boundary-Refined Prototype Generation: A General End-to-End Paradigm for Semi-Supervised Semantic Segmentation. (arXiv:2307.10097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.10097">http://arxiv.org/abs/2307.10097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.10097] Boundary-Refined Prototype Generation: A General End-to-End Paradigm for Semi-Supervised Semantic Segmentation](http://arxiv.org/abs/2307.10097) #segmentation</code></li>
<li>Summary: <p>Prototype-based classification is a classical method in machine learning, and
recently it has achieved remarkable success in semi-supervised semantic
segmentation. However, the current approach isolates the prototype
initialization process from the main training framework, which appears to be
unnecessary. Furthermore, while the direct use of K-Means algorithm for
prototype generation has considered rich intra-class variance, it may not be
the optimal solution for the classification task. To tackle these problems, we
propose a novel boundary-refined prototype generation (BRPG) method, which is
incorporated into the whole training framework. Specifically, our approach
samples and clusters high- and low-confidence features separately based on a
confidence threshold, aiming to generate prototypes closer to the class
boundaries. Moreover, an adaptive prototype optimization strategy is introduced
to make prototype augmentation for categories with scattered feature
distributions. Extensive experiments on the PASCAL VOC 2012 and Cityscapes
datasets demonstrate the superiority and scalability of the proposed method,
outperforming the current state-of-the-art approaches. The code is available at
xxxxxxxxxxxxxx.
</p></li>
</ul>
<h3>Title: Two Approaches to Supervised Image Segmentation. (arXiv:2307.10123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.10123">http://arxiv.org/abs/2307.10123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.10123] Two Approaches to Supervised Image Segmentation](http://arxiv.org/abs/2307.10123) #segmentation</code></li>
<li>Summary: <p>Though performed almost effortlessly by humans, segmenting 2D gray-scale or
color images in terms of their constituent regions of interest
(e.g.~background, objects or portions of objects) constitutes one of the
greatest challenges in science and technology as a consequence of the involved
dimensionality reduction(3D to 2D), noise, reflections, shades, and occlusions,
among many other possible effects. While a large number of interesting
approaches have been respectively suggested along the last decades, it was
mainly with the more recent development of deep learning that more effective
and general solutions have been obtained, currently constituting the basic
comparison reference for this type of operation. Also developed recently, a
multiset-based methodology has been described that is capable of encouraging
performance that combines spatial accuracy, stability, and robustness while
requiring minimal computational resources (hardware and/or training and
recognition time). The interesting features of the latter methodology mostly
follow from the enhanced selectivity and sensitivity, as well as good
robustness to data perturbations and outliers, allowed by the coincidence
similarity index on which the multiset approach to supervised image
segmentation is based. After describing the deep learning and multiset
approaches, the present work develops two comparison experiments between them
which are primarily aimed at illustrating their respective main interesting
features when applied to the adopted specific type of data and parameter
configurations. While the deep learning approach confirmed its potential for
performing image segmentation, the alternative multiset methodology allowed for
encouraging accuracy while requiring little computational resources.
</p></li>
</ul>
<h2>3d point cloud</h2>
<h2>railway infrastructure</h2>
<h2>point cloud segmentation</h2>
<h2>extraction</h2>
<h3>Title: Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline. (arXiv:2307.09821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09821">http://arxiv.org/abs/2307.09821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09821] Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline](http://arxiv.org/abs/2307.09821) #extraction</code></li>
<li>Summary: <p>In dyadic speaker-listener interactions, the listener's head reactions along
with the speaker's head movements, constitute an important non-verbal semantic
expression together. The listener Head generation task aims to synthesize
responsive listener's head videos based on audios of the speaker and reference
images of the listener. Compared to the Talking-head generation, it is more
challenging to capture the correlation clues from the speaker's audio and
visual information. Following the ViCo baseline scheme, we propose a
high-performance solution by enhancing the hierarchical semantic extraction
capability of the audio encoder module and improving the decoder part, renderer
and post-processing modules. Our solution gets the first place on the official
leaderboard for the track of listening head generation. This paper is a
technical report of ViCo@2023 Conversational Head Generation Challenge in ACM
Multimedia 2023 conference.
</p></li>
</ul>
<h2>lidar</h2>
<h3>Title: Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction. (arXiv:2307.09555v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09555">http://arxiv.org/abs/2307.09555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09555] Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction](http://arxiv.org/abs/2307.09555) #lidar</code></li>
<li>Summary: <p>Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling
scene appearance and geometry from multiview imagery. Recent work has also
begun to explore how to use additional supervision from lidar or depth sensor
measurements in the NeRF framework. However, previous lidar-supervised NeRFs
focus on rendering conventional camera imagery and use lidar-derived point
cloud data as auxiliary supervision; thus, they fail to incorporate the
underlying image formation model of the lidar. Here, we propose a novel method
for rendering transient NeRFs that take as input the raw, time-resolved photon
count histograms measured by a single-photon lidar system, and we seek to
render such histograms from novel views. Different from conventional NeRFs, the
approach relies on a time-resolved version of the volume rendering equation to
render the lidar measurements and capture transient light transport phenomena
at picosecond timescales. We evaluate our method on a first-of-its-kind dataset
of simulated and captured transient multiview scans from a prototype
single-photon lidar. Overall, our work brings NeRFs to a new dimension of
imaging at transient timescales, newly enabling rendering of transient imagery
from novel views. Additionally, we show that our approach recovers improved
geometry and conventional appearance compared to point cloud-based supervision
when training on few input viewpoints. Transient NeRFs may be especially useful
for applications which seek to simulate raw lidar measurements for downstream
tasks in autonomous driving, robotics, and remote sensing.
</p></li>
</ul>
<h3>Title: Density-invariant Features for Distant Point Cloud Registration. (arXiv:2307.09788v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.09788">http://arxiv.org/abs/2307.09788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.09788] Density-invariant Features for Distant Point Cloud Registration](http://arxiv.org/abs/2307.09788) #lidar</code></li>
<li>Summary: <p>Registration of distant outdoor LiDAR point clouds is crucial to extending
the 3D vision of collaborative autonomous vehicles, and yet is challenging due
to small overlapping area and a huge disparity between observed point
densities. In this paper, we propose Group-wise Contrastive Learning (GCL)
scheme to extract density-invariant geometric features to register distant
outdoor LiDAR point clouds. We mark through theoretical analysis and
experiments that, contrastive positives should be independent and identically
distributed (i.i.d.), in order to train densityinvariant feature extractors. We
propose upon the conclusion a simple yet effective training scheme to force the
feature of multiple point clouds in the same spatial location (referred to as
positive groups) to be similar, which naturally avoids the sampling bias
introduced by a pair of point clouds to conform with the i.i.d. principle. The
resulting fully-convolutional feature extractor is more powerful and
density-invariant than state-of-the-art methods, improving the registration
recall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and
26.9%, respectively. The code will be open-sourced.
</p></li>
</ul>
<h2>Infrastructure information models</h2>
<h2>edge regularization</h2>
<h2>lod</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-07-20]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
